<!DOCTYPE html>
<html>

<head>

    <!-- meta.html -->
    <!-- Set character encoding for the document -->
    <meta charset="utf-8">
    <!-- Something about IE -->
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <!-- Make the site fit to different screens -->
    <meta name="viewport" content="widh=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- For web crawlers -->
    <meta name="robots" content="index,follow,noodp">  <!-- follow me -->
    <meta name="rating" content="General">  <!-- check my site age -->
    <meta name="subject" content="Deep learning and neural networks">
    <meta name=”description” content=”A blog about deep learning and neural networks”>
    <title></title> 
    <link rel="stylesheet" type="text/css" href="/css/main.css">
    <!-- end meta.html -->


    <!-- analytics.html -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-38506466-1', 'jmcglone.com');
      ga('send', 'pageview');
    </script>
    <!-- end analytics.html -->


    <!-- favicons.html -->
    <!-- For old, favicon.ico in root (16x16, 32x32, 48x48) -->
    <!-- For IE 11, Chrome, Firefox, Safari, Opera -->  
    <link rel="icon" href="/images/favicons/favicon-16.png" sizes="16x16" type="image/png">  
    <link rel="icon" href="/images/favicons/favicon-32.png" sizes="32x32" type="image/png">
    <!-- For phone home screen shortcuts -->
    <link rel="icon" href="/images/favicons/favicon-48.png" sizes="48x48" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-57.png" sizes="57x57" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-62.png" sizes="62x62" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-76.png" sizes="76x76" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-96.png" sizes="96x96" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-120.png" sizes="120x120" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-128.png" sizes="128x128" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-144.png" sizes="144x144" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-152.png" sizes="152x152" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-180.png" sizes="180x180" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-195.png" sizes="195x195" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-196.png" sizes="196x196" type="image/png">
    <link rel="icon" href="/images/favicons/favicon-228.png" sizes="228x228" type="image/png">
    <!-- For windows live tiles -->
    <link rel="icon" href="/images/favicons/smalltile.png" sizes="128x128" type="image/png">
    <link rel="icon" href="/images/favicons/mediumtile.png" sizes="270x270" type="image/png">
    <link rel="icon" href="/images/favicons/widetile.png" sizes="558x270" type="image/png">
    <link rel="icon" href="/images/favicons/largetile.png" sizes="558x558" type="image/png">
    <!--  end favicons.html -->


    <!-- author.html -->
    <link rel="author" href="humans.txt">
    <link rel="me" href="https://twitter.com/mattpetersen_ai" type="text/html">
    <link rel="me" href="mailto:peterm0273@gmail.com">
    <!-- end author.html -->

</head>

<body>


<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

	<title>Matt Petersen's Blog</title>
	<link href="http://mattpetersen.github.io/blog/atom.xml" rel="self"/>
	<link href="http://mattpetersen.github.io/blog"/>
	<updated>2017-06-23T21:09:48-04:00</updated>
	<id>http://mattpetersen.github.io/blog</id>
	<author>
		<name>Matt Petersen</name>
		<email>peterm0273@gmail.com</email>
	</author>

	
		<entry>
			<title>Softmax with cross-entropy</title>
			<link href="http://mattpetersen.github.io/softmax-with-cross-entropy"/>
			<updated>2017-06-23T00:00:00-04:00</updated>
			<id>http://mattpetersen.github.io/softmax-with-cross-entropy</id>
			<content type="html">&lt;p&gt;A matrix-calculus approach to deriving the sensitivity of cross-entropy cost to the weighted input to a softmax output layer. &lt;em&gt;We use row vectors and row gradients&lt;/em&gt;, since typical neural network formulations let columns correspond to features, and rows correspond to examples. This means that the input to our softmax layer is a row vector with a column for each class.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;softmax&quot;&gt;Softmax&lt;/h2&gt;

&lt;p&gt;Softmax is a vector-to-vector transformation that turns a row vector&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf x = \begin{bmatrix}
x_1 \ \ \, 
x_2 \ \ \,
... \ \ \,
x_n \end{bmatrix}
&lt;/script&gt;

&lt;p&gt;into a normalized row vector&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf s(\mathbf x) = \begin{bmatrix}
s(\mathbf x)_1 \ \ \,
s(\mathbf x)_2 \ \ \,
...            \ \ \,
s(\mathbf x)_n \end{bmatrix}
&lt;/script&gt;

&lt;p&gt;The transformation is described element-wise, where the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th output &lt;script type=&quot;math/tex&quot;&gt;s(\mathbf x)_i&lt;/script&gt; is a function of the entire input &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt;, and is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
s(\mathbf x)_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
&lt;/script&gt;

&lt;p&gt;Softmax is nice because it turns &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt; into a probability distribution.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each element &lt;script type=&quot;math/tex&quot;&gt;s(\mathbf x)_i&lt;/script&gt; is between &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The elements &lt;script type=&quot;math/tex&quot;&gt;s(\mathbf x)_i&lt;/script&gt; sum to &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;jacobian-of-softmax&quot;&gt;Jacobian of softmax&lt;/h2&gt;

&lt;p&gt;Since softmax is a vector-to-vector transformation, its derivative is a Jacobian matrix. The Jacobian has a row for each output element &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt;, and a column for each input element &lt;script type=&quot;math/tex&quot;&gt;x_j&lt;/script&gt;. The entries of the Jacobian take two forms, one for the main diagonal entry, and one for every off-diagonal entry. We’ll compute row &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; of the Jacobian, which is the gradient of output element &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; with respect to each of its input elements &lt;script type=&quot;math/tex&quot;&gt;x_j&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;First compute the diagonal entry of row &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; of the Jacobian, that is, compute the derivative of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;‘th output of softmax, &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt;, with respect to its &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;‘th input, &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{aligned}
\frac{\partial s(\mathbf x)_i}{\partial x_i}
&amp;= \frac{\sum_{j=1}^n e^{x_j} e^{x_i} - e^{x_i} e^{x_i}}{(\sum_{j=1}^n e^{x_j})^2} \\[1.6em]
&amp;= s(\mathbf x)_i - s(\mathbf x)_i^2 \qquad \text{diagonal entry}
\end{aligned}
&lt;/script&gt;

&lt;p&gt;Now compute every off-diagonal entry of row &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; of the Jacobian, that is, compute the derivative of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;‘th output of softmax, &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt;, with respect to its &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;‘th input, &lt;script type=&quot;math/tex&quot;&gt;x_j&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;j \neq i&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{aligned}
\frac{\partial s(\mathbf x)_i}{\partial x_j}
&amp;= \frac{\sum_{j=1}^n e^{x_j} \cdot 0 - e^{x_i} e^{x_j}}{(\sum_{j=1}^n e^{x_j})^2} \\[1.6em]
&amp;= - s(\mathbf x)_i s(\mathbf x)_j \qquad \text{off-diagonal entry}
\end{aligned}
&lt;/script&gt;

&lt;p&gt;From now on, to keep things clear, we won’t write dependence on &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt;. Instead we’ll write &lt;script type=&quot;math/tex&quot;&gt;\mathbf s(\mathbf x)&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;\mathbf s&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;s(\mathbf x)_i&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt;, understanding that &lt;script type=&quot;math/tex&quot;&gt;\mathbf s&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; are each a function of the entire vector &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The form of the off-diagonals tells us that the Jacobian of softmax is a symmetric matrix. This is nice because symmetric matrices have great numeric and analytic properties. We expand it below. Each row is a gradient of one output element &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; with respect to each of its input elements &lt;script type=&quot;math/tex&quot;&gt;x_j&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf J_{\mathbf x}(\mathbf s) = \begin{bmatrix}
s_0 - s_0^2 &amp;  -s_0 s_1   &amp;    ...   &amp;  -s_0 s_n     \\[0.5em]
 -s_1 s_0   &amp; s_1 - s_1^2 &amp;    ...   &amp;  -s_1 s_n     \\[0.5em]
   ...      &amp;    ...      &amp;    ...   &amp;    ...        \\[0.5em]
 -s_n s_0   &amp;  -s_n s_1   &amp;    ...   &amp; s_n - s_n^2   \end{bmatrix}
&lt;/script&gt;

&lt;p&gt;Notice that we can express this matrix as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf J_{\mathbf x}(\mathbf s)
= \text{diag}(\mathbf s) - \mathbf s^\top \mathbf s
&lt;/script&gt;

&lt;p&gt;where the second term is the &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; outer product, because we defined &lt;script type=&quot;math/tex&quot;&gt;\mathbf s&lt;/script&gt; as a row vector.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;cross-entropy&quot;&gt;Cross-entropy&lt;/h2&gt;

&lt;p&gt;Cross-entropy measures the difference between two probability distributions. We saw that &lt;script type=&quot;math/tex&quot;&gt;\mathbf s&lt;/script&gt; is a distribution. The correct class is also a distribution if we encode it as a one-hot vector:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \begin{aligned}
\mathbf y &amp;= \begin{bmatrix}
        y_1  \ \ \,
        y_2  \ \ \,
        ...  \ \ \,
        y_n  \end{bmatrix} \\[1.1em]
&amp;=  \begin{bmatrix}
0   \ \ \,
0   \ \ \,
... \ \ \,
1   \ \ \,
... \ \ \,
0   \end{bmatrix}
\end{aligned} &lt;/script&gt;

&lt;p&gt;where the &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; appears at the index of the correct class of this single example.&lt;/p&gt;

&lt;p&gt;The cross-entropy between our predicted distribution over classes, &lt;script type=&quot;math/tex&quot;&gt;\mathbf s&lt;/script&gt;, and the true distribution over classes, &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt;, is a scalar measure of their difference, which is perfect for a cost function. It’ll drive our softmax distribution toward the one-hot distribution. We can write this cost function as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{aligned}
H(\mathbf y, \mathbf s)
&amp;= -\sum_{i=1}^n y_i \log s_i \\[1.6em] 
&amp;= -\mathbf y \log \mathbf s^\top
\end{aligned}
&lt;/script&gt;

&lt;p&gt;which is the dot product since we’re using row vectors. This formula comes from information theory. It measures the information gained about our softmax distribution when we sample from our one-hot distribution.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;gradient-of-cross-entropy&quot;&gt;Gradient of cross-entropy&lt;/h2&gt;

&lt;p&gt;Since our &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt; is given and fixed, cross-entropy is a vector-to-scalar function of only our softmax distribution. That means it will have a gradient with respect to our softmax distribution. This vector-to-scalar cost function is actually made up of two steps: (1) a vector-to-vector element-wise &lt;script type=&quot;math/tex&quot;&gt;\log&lt;/script&gt; and (2) a vector-to-scalar dot product. The vector-to-vector logarithm will have a Jacobian, but since it’s applied element-wise, the Jacobian will be diagonal, holding each elementwise derivative. The gradient of a dot product, being a linear operation, is just the vector &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{aligned}
\nabla_{\mathbf s} H 
&amp;= -\nabla_{\mathbf s} \mathbf y \log \mathbf s^\top                \\[1.6em]
&amp;= -\mathbf y \nabla_{\mathbf s} \log \mathbf s                     \\[1.1em]
&amp;= -\mathbf y \ \text{diag}\left(\frac{\mathbf 1}{\mathbf s}\right) \\[1.6em]
&amp;= -\frac{\mathbf y}{\mathbf s}
\end{aligned}
&lt;/script&gt;

&lt;p&gt;where we used equation (69) of &lt;a href=&quot;http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf&quot;&gt;the matrix cookbook&lt;/a&gt; for the derivative of the dot product.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;finishing-up-with-the-chain-rule&quot;&gt;Finishing up with the chain rule&lt;/h2&gt;

&lt;p&gt;By the chain rule, the sensitivity of cost &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; to the input to the softmax layer, &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt;, is given by a gradient-Jacobian product, each of which we’ve already computed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\nabla_{\mathbf x} H = \nabla_{\mathbf s} H \ \mathbf J_{\mathbf x}(\mathbf s)
&lt;/script&gt;

&lt;p&gt;The first term is the gradient of cross-entropy cost to softmax activation. The second term is the Jacobian of softmax activation to softmax input. Remember that we’re using row gradients - this is a row vector times a matrix, resulting in a row vector. Expanding and simplifying, we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \begin{aligned}
\nabla_{\mathbf x} H
&amp;= -\frac{\mathbf y}{\mathbf s} 
    \bigg[\text{diag}(\mathbf s)
   -\mathbf s^\top \mathbf s \bigg]                         \\[1.6em]
&amp;= \frac{\mathbf y}{\mathbf s} \mathbf s^\top \mathbf s
 - \frac{\mathbf y}{\mathbf s} \ \text{diag}(\mathbf s)     \\[1.6em]
&amp;= \mathbf y \ \mathbf S^{\text{repeated row}} - \mathbf y  \\[1.6em]
&amp;= \mathbf s - \mathbf y
\end{aligned} &lt;/script&gt;

&lt;p&gt;The last line follows from the fact that &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt; was one-hot and applied to a matrix whose rows are identically our softmax distribution. But actually, any &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt; whose elements sum to &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; would satisfy the same property. To be more specific, the equation above would hold not just for one-hot &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt;, but for any &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt; specifying a distribution over classes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;now-with-a-batch-of-examples&quot;&gt;Now with a batch of examples&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Our work thus far considered a single example. Hence &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt;, our input to the softmax layer, was a row vector. Alternatively, if we feed forward a batch of &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; examples, then &lt;script type=&quot;math/tex&quot;&gt;\mathbf X&lt;/script&gt; contains a row vector for each example in the minibatch.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf X =   \begin{bmatrix}
\mathbf x_1   \\[0.4em]
\mathbf x_2   \\[0.4em]
        ...   \\[0.4em]
\mathbf x_m   \end{bmatrix} \sim m \times n
&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;softmax-1&quot;&gt;Softmax&lt;/h2&gt;

&lt;p&gt;Softmax is still a vector-to-vector transformation, but it’s applied independently to each row of &lt;script type=&quot;math/tex&quot;&gt;\mathbf X&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf S =              \begin{bmatrix}
\mathbf s(\mathbf x_1)   \\[0.6em]
\mathbf s(\mathbf x_2)   \\[0.6em]
        ...              \\[0.6em]
\mathbf s(\mathbf x_m)   \end{bmatrix} \sim m \times n
&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;jacobian-of-softmax-1&quot;&gt;Jacobian of softmax&lt;/h2&gt;
&lt;p&gt;Because rows are independently mapped, the Jacobian of row &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\mathbf S&lt;/script&gt; with respect to row &lt;script type=&quot;math/tex&quot;&gt;j \neq i&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\mathbf X&lt;/script&gt; is a zero matrix.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf J_{\mathbf x_{j \neq i}}(\mathbf s_i) = \mathbf 0
&lt;/script&gt;

&lt;p&gt;and the Jacobian of row &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\mathbf S&lt;/script&gt; with respect to row &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\mathbf X&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf J_{\mathbf x_{i}}(\mathbf s_i) = \text{diag}(\mathbf s_i) - \mathbf s_i^\top \mathbf s_i
&lt;/script&gt;

&lt;p&gt;That means our Jacobian of &lt;script type=&quot;math/tex&quot;&gt;\mathbf S&lt;/script&gt; with respect to &lt;script type=&quot;math/tex&quot;&gt;\mathbf X&lt;/script&gt; is a diagonal &lt;script type=&quot;math/tex&quot;&gt;m \times m&lt;/script&gt; matrix of &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; matrices.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf J_{\mathbf X}(\mathbf S) = \begin{bmatrix}
\mathbf J_{\mathbf x_1}(\mathbf s_1) &amp; \mathbf 0 &amp; ... &amp; \mathbf 0 \\[1.1em]
\mathbf 0 &amp; \mathbf J_{\mathbf x_2}(\mathbf s_2) &amp; ... &amp; \mathbf 0 \\[1.1em]
... &amp; ... &amp; ... &amp; ...                                              \\[1.1em]
\mathbf 0 &amp; \mathbf 0 &amp; ... &amp; \mathbf J_{\mathbf x_m}(\mathbf s_m)
\end{bmatrix} &lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;cross-entropy-1&quot;&gt;Cross-entropy&lt;/h2&gt;

&lt;p&gt;Let each row of &lt;script type=&quot;math/tex&quot;&gt;\mathbf Y&lt;/script&gt; be a one-hot label for an example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf Y =   \begin{bmatrix}
\mathbf y_1   \\[0.4em]
\mathbf y_2   \\[0.4em]
        ...   \\[0.4em]
\mathbf y_m   \end{bmatrix} \sim m \times n
&lt;/script&gt;

&lt;p&gt;Then we compute the &lt;em&gt;mean cross-entropy&lt;/em&gt; by averaging the cross-entropy of every matching pair of rows of &lt;script type=&quot;math/tex&quot;&gt;\mathbf Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf S&lt;/script&gt;. That is, we average over examples, the cross-entropy of each example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \begin{aligned}
H(\mathbf Y, \mathbf S)
&amp;= -\frac{1}{m} \sum_{i=1}^m \mathbf y_i \log \mathbf s_i^\top  \\[1.6em]
&amp;= -\frac{1}{m} \text{Tr}(\mathbf Y \log \mathbf S^\top)
\end{aligned} &lt;/script&gt;

&lt;p&gt;The above simplification works because each row of &lt;script type=&quot;math/tex&quot;&gt;\mathbf S&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\mathbf s_i&lt;/script&gt;. So each column of &lt;script type=&quot;math/tex&quot;&gt;\mathbf S^\top&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\mathbf s_i&lt;/script&gt;. So the matrix product &lt;script type=&quot;math/tex&quot;&gt;\mathbf Y \log \mathbf S^\top&lt;/script&gt; dots rows of &lt;script type=&quot;math/tex&quot;&gt;\mathbf Y&lt;/script&gt; with columns of &lt;script type=&quot;math/tex&quot;&gt;(\log \mathbf S^\top)&lt;/script&gt;, which is exactly what we want for cross-entropy. Now, we only care about entries where the row index equals the column index. That’s because cross-entropy sums the dot products of &lt;em&gt;matching&lt;/em&gt; rows of &lt;script type=&quot;math/tex&quot;&gt;\mathbf Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf S&lt;/script&gt;. We can sum over matching dot products by using a trace.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; this formulation is computationally wasteful. We shouldn’t implement batch cross-entropy this way in a computer. We’re only using it for its analytic simplicity to work out the backpropogating error. However, the end analytic result is actually computationally efficient.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;gradient-of-cross-entropy-1&quot;&gt;Gradient of cross-entropy&lt;/h2&gt;

&lt;p&gt;Since mean cross-entropy maps a matrix to a scalar, its Jacobian with respect to &lt;script type=&quot;math/tex&quot;&gt;\mathbf S&lt;/script&gt; will be a matrix.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \begin{aligned}
\mathbf J_{\mathbf S}(H) 
&amp;= \frac{\partial}{\partial \mathbf S} H                              \\[1.6em]
&amp;= -\frac{\partial}{\partial \mathbf S}
   \bigg(\frac{1}{m} \text{Tr}(\mathbf Y \log \mathbf S^\top) \bigg)  \\[1.6em]
&amp;= -\frac{1}{m} \frac{\partial}{\partial \mathbf S}
   \Big(\text{Tr}(\mathbf Y \log \mathbf S^\top) \Big)                \\[1.6em]
&amp;= -\frac{1}{m} \mathbf Y
   \frac{\partial}{\partial \mathbf S} \Big(\log \mathbf S \Big)      \\[1.6em]
&amp;= -\frac{1}{m} \mathbf Y \odot \frac{1}{\mathbf S}                   \\[1.6em]
&amp;= -\frac{1}{m} \frac{\mathbf Y}{\mathbf S}
\end{aligned} &lt;/script&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;\log \mathbf S&lt;/script&gt; is an element-wise operation mapping a matrix to a matrix, its Jacobian is a matrix of element-wise derivatives which we chain rule by a hadamard product, rather than a dot product.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;why-this-works&quot;&gt;Why this works&lt;/h4&gt;

&lt;p&gt;This procedure is always true for any element-wise operations. We can see this by concatenating the rows of &lt;script type=&quot;math/tex&quot;&gt;\mathbf S&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf s = \begin{bmatrix}
\mathbf s_1   \ \,
\mathbf s_2   \ \,
        ...   \ \,
\mathbf s_m   \end{bmatrix}
&lt;/script&gt;

&lt;p&gt;such that &lt;script type=&quot;math/tex&quot;&gt;\mathbf s&lt;/script&gt; is a row vector of length &lt;script type=&quot;math/tex&quot;&gt;m \cdot n&lt;/script&gt;. Then &lt;script type=&quot;math/tex&quot;&gt;\log&lt;/script&gt; is an element-wise vector-to-vector transformation again. So it has an &lt;script type=&quot;math/tex&quot;&gt;m \cdot n \times m \cdot n&lt;/script&gt; diagonal Jacobian matrix.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf J_{\mathbf s}(\log \mathbf s) =
\text{diag}\left(\frac{\mathbf 1}{\mathbf s}\right)
&lt;/script&gt;

&lt;p&gt;If we flatten &lt;script type=&quot;math/tex&quot;&gt;\mathbf Y&lt;/script&gt; in the same way&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf y = \begin{bmatrix}
\mathbf y_1   \ \,
\mathbf y_2   \ \,
        ...   \ \,
\mathbf y_m   \end{bmatrix}
&lt;/script&gt;

&lt;p&gt;then we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
H(\mathbf y, \mathbf s) = - \frac{1}{m} \mathbf y \log \mathbf s^\top
&lt;/script&gt;

&lt;p&gt;and so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \begin{aligned}
\mathbf J_{\mathbf s}(H)
&amp;= -\frac{1}{m} \mathbf y
   \frac{\partial}{\partial \mathbf s} \Big(\log \mathbf s \Big)      \\[1.6em]
&amp;= -\frac{1}{m} \mathbf y
   \text{diag}\left(\frac{\mathbf 1}{\mathbf s}\right)                \\[1.6em]
&amp;= -\frac{1}{m} \frac{\mathbf y}{\mathbf s}
\end{aligned} &lt;/script&gt;

&lt;p&gt;Now since &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf s&lt;/script&gt; are each of length &lt;script type=&quot;math/tex&quot;&gt;m \cdot n&lt;/script&gt;, we can reshape this formulation back into matrices, understanding that in both cases the division is element-wise:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\mathbf J_{\mathbf s}(H) = -\frac{1}{m} \frac{\mathbf Y}{\mathbf S}
&lt;/script&gt;

&lt;p&gt;and we have our result. &lt;script type=&quot;math/tex&quot;&gt;\square&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;finishing-up-with-the-chain-rule-1&quot;&gt;Finishing up with the chain rule&lt;/h2&gt;

&lt;p&gt;We apply the chain rule just as before. The only difference is that our gradient-Jacobian product is now a matrix-tensor product.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \begin{aligned}
\nabla_{\mathbf X} H
&amp;= \mathbf J_{\mathbf S}(H) \ \mathbf J_{\mathbf X}(\mathbf S)    \\[1.6em]
&amp;= \bigg(-\frac{1}{m} \frac{\mathbf Y}{\mathbf S}\bigg)
   \bigg(\mathbf J_{\mathbf X}(\mathbf S) \bigg)                  \\[2.4em]
&amp;= \frac{1}{m} \odot \begin{bmatrix}
-\mathbf y_1 / \mathbf s_1  \\[0.4em]
-\mathbf y_2 / \mathbf s_2  \\[0.4em]
...                        \\[0.4em]
-\mathbf y_m / \mathbf s_m
\end{bmatrix}
\begin{bmatrix}
\mathbf J_{\mathbf x_1}(\mathbf s_1) &amp; \mathbf 0 &amp; ... &amp; \mathbf 0    \\[0.4em]
\mathbf 0 &amp; \mathbf J_{\mathbf x_2}(\mathbf s_2) &amp; ... &amp; \mathbf 0    \\[0.4em]
... &amp; ... &amp; ... &amp; ...                                                 \\[0.4em]
\mathbf 0 &amp; \mathbf 0 &amp; ... &amp; \mathbf J_{\mathbf x_m}(\mathbf s_m)
\end{bmatrix}                                                         \\[0.4em]
\\[1.1em]
&amp;= \frac{1}{m} \odot \begin{bmatrix}
-\frac{\mathbf y_1}{\mathbf s_1} \mathbf J_{\mathbf x_1}(\mathbf s_1)  \\[1.4em]
-\frac{\mathbf y_2}{\mathbf s_2} \mathbf J_{\mathbf x_2}(\mathbf s_2)  \\[1.4em]
...                                                                   \\[1.4em]
-\frac{\mathbf y_m}{\mathbf s_m} \mathbf J_{\mathbf x_m}(\mathbf s_m)
\end{bmatrix}                                                         \\[1.4em]
\\[1.1em]
&amp;= \frac{1}{m} \odot       \begin{bmatrix}
\mathbf s_1 - \mathbf y_1  \\[0.4em]
\mathbf s_2 - \mathbf y_2  \\[0.4em]
...                        \\[0.4em]
\mathbf s_m - \mathbf y_m  \end{bmatrix}                              \\[1.4em]
\\[1.1em]
&amp;= \frac{1}{m} \odot \Big(\mathbf S - \mathbf Y\Big)
\end{aligned} &lt;/script&gt;

&lt;p&gt;So the sensitivity of cost to the weighted input to our softmax layer is just the difference of our softmax matrix and our matrix of one-hot labels, where every element is divided by the number of examples in the batch. &lt;script type=&quot;math/tex&quot;&gt;\quad \square&lt;/script&gt;&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>Tidal music player for Linux</title>
			<link href="http://mattpetersen.github.io/tidal-music-player-for-linux"/>
			<updated>2017-06-21T00:00:00-04:00</updated>
			<id>http://mattpetersen.github.io/tidal-music-player-for-linux</id>
			<content type="html">&lt;p&gt;I found an awesome package called &lt;a href=&quot;https://github.com/Bunkerbewohner/tidal-music-linux&quot;&gt;tidal-music-linux&lt;/a&gt; that wraps a Chromium web player in an Electron shell for a standalone Tidal player like on Windows. It’s written by &lt;a href=&quot;https://github.com/Bunkerbewohner&quot;&gt;Mathias Kahl&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/tidal-music-player/screenshot.png&quot; style=&quot;width: 100%; object-fit: contain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It should work in either Ubuntu or Arch. A major drawback is that I haven’t gotten HiFi playback to work. It says HiFi is only allowed in Chrome. Maybe wrapping a Chrome player in Electron rather than a Chromium player would solve this. Regardless, here’s how to install if you’re okay with 320 kbps playback in a beautiful standalone application.&lt;/p&gt;

&lt;h2 id=&quot;arch-instructions&quot;&gt;Arch Instructions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Install &lt;a href=&quot;https://aur.archlinux.org/packages/pepper-flash/&quot;&gt;Pepper Flash Player&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;yaourt -S pepper-flash&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Install &lt;a href=&quot;https://get.adobe.com/flashplayer/&quot;&gt;Adobe Flash Player&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;Download the .tar.gz bundle from the link above&lt;/li&gt;
      &lt;li&gt;Move the .tar.gz bundle to &lt;code class=&quot;highlighter-rouge&quot;&gt;usr/lib/adobe-flashplugin&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir /usr/lib/adobe-flashplugin&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mv ~/Downloads/flash_player_ppapi_linux.x86_64.tar.gz /usr/lib/adobe-flashplugin&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Install &lt;a href=&quot;https://www.archlinux.org/packages/community/any/npm/&quot;&gt;Node Package Manager (NPM)&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pacman -S npm&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Download
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git clone https://github.com/Bunkerbewohner/tidal-music-linux&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Change name
    &lt;ul&gt;
      &lt;li&gt;Inside package.json the name “Tidal music - Linux” creates an install error, so change it to something like “tidal-music-linux”
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gedit tidal-music-linux/package.json&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Change the second line to “name”: “tidal-music-linux”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Install
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd tidal-music-linux&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;npm install&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Run
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;npm start&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
		</entry>
	
		<entry>
			<title>Are lefties really smarter?</title>
			<link href="http://mattpetersen.github.io/are-lefties-really-smarter"/>
			<updated>2017-06-17T00:00:00-04:00</updated>
			<id>http://mattpetersen.github.io/are-lefties-really-smarter</id>
			<content type="html">&lt;p&gt;There’s &lt;a href=&quot;https://www.indy100.com/article/left-handed-people-smarter-science-ifl-science-maths-7797656&quot;&gt;a viral article&lt;/a&gt; today that the science is finally settled and left-handed people are smarter than right-handed people. There are good reasons to be skeptical. For those curious, the source paper can be found &lt;a href=&quot;http://journal.frontiersin.org/article/10.3389/fpsyg.2017.00948/full&quot;&gt;here&lt;/a&gt;. I’m not a neuroscientist so if I’m mistaken anywhere I would love to hear from you.&lt;/p&gt;

&lt;h3 id=&quot;1---virality&quot;&gt;1 - Virality&lt;/h3&gt;

&lt;p&gt;Any story that’s perfectly poised to make the internet do backflips deserves an extra ounce of skepticism. False positives tend to be &lt;a href=&quot;https://www.scientificamerican.com/article/an-epidemic-of-false-claims/&quot;&gt;over-represented in viral studies&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;2---vague-assertions&quot;&gt;2 - Vague assertions&lt;/h3&gt;

&lt;p&gt;The justification for the finding was that hand function is&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“a manifestation of brain function and is therefore related to cognition.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is so broad a statement as to be meaningless. Every action is a manifestation of brain function. Is a manifestation of brain function necessarily related to cognition? Is cognition necessarily related to intelligence? We have a lot of bridges to build here.&lt;/p&gt;

&lt;h3 id=&quot;3---false-assumptions&quot;&gt;3 - False assumptions&lt;/h3&gt;

&lt;p&gt;While it is true that &lt;a href=&quot;http://science.sciencemag.org/content/229/4714/665.long&quot;&gt;lefties have more connections between brain hemispheres&lt;/a&gt;, a property known as &lt;em&gt;lateralization&lt;/em&gt;, there is actually &lt;a href=&quot;http://www.pnas.org/content/110/36/E3435.full.pdf&quot;&gt;no significant link between lateralization and brain function&lt;/a&gt;. This may seem counterintuitive, because &lt;a href=&quot;https://en.wikipedia.org/wiki/Talk%3ALobotomy#IQ_drop&quot;&gt;labotomized patients lose 9.2 to 17 points of IQ&lt;/a&gt;. These facts together point to diminishing returns of lateralization beyond a certain threshold. Our default assumption should be that there’s no IQ benefit from the increased lateralization found in lefties, since we’re comparing against regular people not lobotomized patients.&lt;/p&gt;

&lt;h3 id=&quot;4---mixed-effect-direction&quot;&gt;4 - Mixed effect direction&lt;/h3&gt;

&lt;p&gt;Lefty children perform &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.1467-985X.2012.01074.x/abstract&quot;&gt;worse in math&lt;/a&gt; and &lt;a href=&quot;https://link.springer.com/article/10.1353/dem.0.0053&quot;&gt;“significantly worse in nearly all measures of development”&lt;/a&gt; when compared to righty children. Why should the effect direction reverse upon adulthood? The simplest answer is that one or both effects are spurious.&lt;/p&gt;

&lt;h3 id=&quot;5---overparameterized-model&quot;&gt;5 - Overparameterized model&lt;/h3&gt;

&lt;p&gt;The authors fit a quartic polynomial linear regression with the only justification being its increased expressive power. But with great expressive power comes great responsibility. Try enough models, with enough complexity, and you’ll eventually find one that fits. That means, as we increase the complexity of our model, we’re increasing our chance of false positives.&lt;/p&gt;

&lt;p&gt;I think economists do the best job here. Economists first build a mathematically intuitive, theoretical explanation of phenomena. That mathematical explanation hints at the proper form of the empirical model with which to test the theory. This is a robust scientific method where we first hypothesize, and then test once, as opposed to testing many times and forming a hypothesis from what works. The latter method is prone to overfitting and false positives. The former method is prone to underfitting. In my opinion a good scientist should heir on the side of underfitting, although Erika Salomon makes a great case that overfitting is worth the cost of false positives, because we &lt;a href=&quot;http://www.erikasalomon.com/2015/06/p-hacking-true-effects/&quot;&gt;also get more true positives&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I just wanted to point out some red statistical flags. I don’t mean to slander the authors of the original study, and I’ve not said anything to &lt;em&gt;disprove&lt;/em&gt; their statements, but only to emphasize that we should be careful. If you’re a neuroscientists or someone with in-depth knowledge of the field, I’d be grateful for your setting me straight on anything above. Cheers guys :)&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>Why PhD's work for free</title>
			<link href="http://mattpetersen.github.io/why-phds-work-for-free"/>
			<updated>2017-04-05T00:00:00-04:00</updated>
			<id>http://mattpetersen.github.io/why-phds-work-for-free</id>
			<content type="html">&lt;p&gt;This is an informal post, so feel free to chime in with comments and corrections. I was talking with a friend last night about the shocking fact that welders and truckers often out-earn PhD holders. It was then that I realized something profound, which I think a lot of people overlook - &lt;em&gt;wages depend on scarcity, not skill&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This goes against our intuition, and for good reason. Throughout history, skilled labor was in short supply. Employers bargained against each other for those prestigious few who held advanced degrees, and this bargaining led to higher wages.&lt;/p&gt;

&lt;p&gt;In modern times the story is completely changing. With the entrance of China and India to the global stage, we’ve seen a glut of skilled labor worldwide. This glut has been expanded by the American mantra that children go to university, a decision that, historically, had always led to scarcer skills and higher wages. I’ll argue that this is no longer the case for a growing range of fields.&lt;/p&gt;

&lt;h2 id=&quot;the-first-signs-of-an-education-epidemic&quot;&gt;The first signs of an education epidemic&lt;/h2&gt;

&lt;p&gt;The glut of skilled labor first showed itself in the burgeoning unemployment of non-technical university graduates, those in arts and humanities. The problem has two sources. Firstly, universities have increased in number, and in enrollment. This is normal and follows market forces. Demand for university was high because graduates held scarce skills. Ballooning enrollment is simply the market acting to cure that scarcity.&lt;/p&gt;

&lt;p&gt;The second reason behind the unemployment of arts and humanities graduates is that learning and employing artistic skill is intrinsically rewarding, so these graduates are more susceptible to working for free. This blow is on top of already graduating from a field with higher natural enrollment due to its allure.&lt;/p&gt;

&lt;p&gt;The lesson is this. Having numerous intrinsically motivated peers is a recipe for low wages, and this effect is now happening in STEM.&lt;/p&gt;

&lt;h2 id=&quot;the-latest-victim&quot;&gt;The latest victim&lt;/h2&gt;

&lt;p&gt;My first encounter with the idea that STEM skills are entering a glut was at a regional data science competition in 2014. A professor told me about a colleague with a PhD in mathematical biology who was driving a semi truck. The pay was higher than jobs that employed his skill, and the solitude suited his personality.&lt;/p&gt;

&lt;p&gt;My second encounter was at a Goldman Sachs presentation at Columbia in 2015. The presenter said something quite harsh along the lines of&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You think your quantitative skills make you special? I can snap my fingers and have 10 pure math PhD’s from MIT lined up for unpaid internships with us. You have to be special beyond your technical skills.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Survivorship bias has convinced many people that this glut doesn’t exist. When I met the data science team at eBay New York, it consisted of two PhD’s in physics who studied quantum properties of graphene, a PhD in number theory who studied spectral properties of matrices, and a PhD in computer science who studied machine learning. Needless to say I didn’t get the position, although their interviewing me means they’re at least open to the prospect of hiring masters graduates.&lt;/p&gt;

&lt;p&gt;Aside from eBay, every data science team in New York that I’ve been blessed to meet has consisted entirely of fantastic PhD’s, and from top schools at that. This sounds like great news for the PhD job market. Unfortunately we’re only seeing the successful tip of the PhD iceberg. Statistics speak to &lt;a href=&quot;http://www.sciencemag.org/careers/2016/06/fool-s-gold-phd-employment-data&quot;&gt;a broader unemployment and underemployment of PhD’s at large&lt;/a&gt;, although it’s hard to quantify the exact job market outcome of each of field of study.&lt;/p&gt;

&lt;h2 id=&quot;historical-precedent&quot;&gt;Historical precedent&lt;/h2&gt;

&lt;p&gt;This is not the first time that a glut of skilled labor has depressed wages. Over the past few decades, the music industry, at least among small musicians, has moved almost entirely to donations. Venues have closed, sales have slowed, and streaming has taken over. This unfortunate trend shares much in common with our glut in education.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Creating music is intrinsically rewarding.&lt;/li&gt;
  &lt;li&gt;Modern wealth affords unprecedented numbers of people to pursue it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two forces have driven wages for musicians to virtual nonexistence, where small artists now survive mostly on &lt;a href=&quot;https://www.patreon.com/&quot;&gt;Patreon&lt;/a&gt; donations.&lt;/p&gt;

&lt;p&gt;A second industry experiencing a glut of skilled workers is online content creation and curation. Blogs, &lt;a href=&quot;https://medium.com/&quot;&gt;Medium&lt;/a&gt; articles, and YouTube personalities are doing for free what news anchors and writers used to make a salary doing. We can again identify our two core reagents driving wages to nonexistence among online content creators.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Creating and curating online content is intrinsically rewarding.&lt;/li&gt;
  &lt;li&gt;Unprecedented numbers of people own the computers and time to do this.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A case example is the increasing number of free online textbooks (see &lt;a href=&quot;http://www.deeplearningbook.org/&quot;&gt;Goodfellow 206&lt;/a&gt; and &lt;a href=&quot;https://sites.ualberta.ca/~szepesva/RLBook.html&quot;&gt;Szepesvari 2013&lt;/a&gt;) and &lt;a href=&quot;http://distill.pub/&quot;&gt;incredibly fantastic educational blogs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The last historical precedent that I can point to is more recent - the plethora of online open-source code. GitHub is the finest example of the willingness of human beings to do labor for nothing more than intrinsic reward. If companies are able to profitably use open-source code, they have less reason to employ full time programmers.&lt;/p&gt;

&lt;h2 id=&quot;in-summary&quot;&gt;In summary&lt;/h2&gt;

&lt;p&gt;Will programmers and mathematicians be the starving artists of tomorrow? Will every form of enjoyable human labor be eventually ground by economic forces toward subsistence wages? This seems to be the case, but perhaps I’m over-fitting to sparse examples. Most of my argument does rely on personal anecdotes after all. Let me know what you think, and if you’ve seen anything similar :)&lt;/p&gt;

</content>
		</entry>
	
		<entry>
			<title>Unity can be beautiful</title>
			<link href="http://mattpetersen.github.io/unity-can-be-beautiful"/>
			<updated>2017-03-19T00:00:00-04:00</updated>
			<id>http://mattpetersen.github.io/unity-can-be-beautiful</id>
			<content type="html">&lt;p&gt;When I started deep learning last year, I installed Ubuntu with the default Unity desktop environment. Unity gets a lot of flak from users of other Linux distributions. I’m writing to dispel some of that flak.&lt;/p&gt;

&lt;p&gt;First off, all Linux desktop environments are massively better than they were even five years ago. If you think Linux is still terminals and outdated design, you’re sorely mistaken.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/unity-can-be-beautiful/my-desktop.jpg&quot; style=&quot;width: 100%; object-fit: contain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Why Linux? If you’re going to be doing deep learning, you’re probably going to be using either Linux or Mac. That’s because developers of deep learning tools prefer writing for Unix-based operating systems. Unfortunately, since Mac is hard to equip with a decent GPU, you’re pretty much stuck with Linux. Sure, Windows &lt;a href=&quot;https://www.tensorflow.org/install/install_windows&quot;&gt;recently got TensorFlow support&lt;/a&gt;, but you’ll find that TensorFlow updates and support for Windows are always a bit behind, and this goes for other deep learning packages too. Furthermore, if you have an issue with a deep learning library on Windows, it can be hard to find help, and to fix. Since library developers use Linux, most of these sort of issues are addressed already, or they have user-written solutions. That said, if you’re still attached to Windows, you can always just &lt;a href=&quot;http://www.everydaylinuxuser.com/2015/11/how-to-install-ubuntu-linux-alongside.html&quot;&gt;dual boot&lt;/a&gt; like I do. If you’re attached to Mac, you can &lt;a href=&quot;https://www.youtube.com/watch?v=eVzYtlR_OH0&quot;&gt;make Ubuntu 16.04 look just like Mac OSX&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;stock-unity-is-ugly&quot;&gt;Stock Unity is ugly&lt;/h2&gt;

&lt;p&gt;At first encounter, Unity is pretty jarring.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://core0.staticworld.net/images/article/2015/12/dash-search-option-100633613-orig.png&quot; style=&quot;width: 100%; object-fit: contain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The icons are humongous, the launcher and docker both use a rounded 3-d theme that hearkens back to 2010, and every window has an ugly header. To my shock, even windows like Spotify and Discord &lt;em&gt;which are supposed to have their own built-in headers&lt;/em&gt; use the awful Ubuntu headers instead. Luckily, you can fix all of this distasteful stuff in just 10 or 20 minutes.&lt;/p&gt;

&lt;h2 id=&quot;move-the-launcher&quot;&gt;Move the launcher&lt;/h2&gt;

&lt;p&gt;You can shrink the launcher from the system menu.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/unity-can-be-beautiful/unity-launcher-size.png&quot; style=&quot;width: 100%; object-fit: contain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can move the launcher to the bottom by using the &lt;em&gt;unity tweak tool&lt;/em&gt;. To install, type&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install unity-tweak-tool gnome-tweak-tool&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then find it in your applications and launch it. The tool should look like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/unity-can-be-beautiful/tweak-tool.png&quot; style=&quot;width: 70%; object-fit: contain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click &lt;strong&gt;launcher&lt;/strong&gt; in the upper left and you’ll see this&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/unity-can-be-beautiful/tweak-tool-launcher.png&quot; style=&quot;width: 70%; object-fit: contain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I like mine on the bottom, auto-hidden, and completely opaque. You can change tons of other Unity features with this tool, but not every feature. To remove window headers, you’ll need one more tool.&lt;/p&gt;

&lt;h2 id=&quot;remove-window-headers&quot;&gt;Remove window headers&lt;/h2&gt;

&lt;p&gt;Removing window headers comes with a few caveats, all of which are worthwhile and more efficient once you get used to them. Firstly, you’ll have to &lt;code class=&quot;highlighter-rouge&quot;&gt;alt + left click&lt;/code&gt; to drag windows, although snap-resizing still works. Secondly, you’ll have to use &lt;code class=&quot;highlighter-rouge&quot;&gt;alt + f4&lt;/code&gt; to close windows, and &lt;code class=&quot;highlighter-rouge&quot;&gt;alt + space&lt;/code&gt; followed by &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; to minimize windows. In fact, &lt;code class=&quot;highlighter-rouge&quot;&gt;alt + space&lt;/code&gt; brings up a small menu with every window adjustment feature that is normally included on window headers. Lastly, in Unity, the title of the window is always displayed in the top bar, so you’re really losing nothing by getting rid of window headers.&lt;/p&gt;

&lt;p&gt;A perk of having no windows headers is the extra screen real estate you get. I like to snap windows to the four corners of my screen by using &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl + alt + 1&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl + alt + 7&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl + alt + 9&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl + alt + 3&lt;/code&gt;, and with each of the four windows having a window header, it looked super cluttered.&lt;/p&gt;

&lt;p&gt;So, to remove window headers, create a file named &lt;code class=&quot;highlighter-rouge&quot;&gt;gtk.css&lt;/code&gt; in the directory &lt;code class=&quot;highlighter-rouge&quot;&gt;home/.config/gtk-3.0&lt;/code&gt;, or &lt;code class=&quot;highlighter-rouge&quot;&gt;home/.config/gtk-2.0&lt;/code&gt; if that’s the only directory listed. I think Ubuntu 12.04 was the first to introduce gtk-3.0, but correct me if I’m wrong. Also, &lt;code class=&quot;highlighter-rouge&quot;&gt;.config&lt;/code&gt; is a hidden directory, so you might need to press &lt;code class=&quot;highlighter-rouge&quot;&gt;ctrl + h&lt;/code&gt; to see it if you’re using a visual file browser. Here’s what my &lt;code class=&quot;highlighter-rouge&quot;&gt;gtk.css&lt;/code&gt; file looks like.&lt;/p&gt;

&lt;pre class=&quot;prettyprint&quot;&gt;
UnityDecoration {
/* One pixel border */
-UnityDecoration-extents: 1px 1px 1px 1px;

/* No shadows */
-UnityDecoration-shadow-offset-x: 0px;
-UnityDecoration-shadow-offset-y: 0px;
-UnityDecoration-active-shadow-radius: 0px;
}


/* Light blue around active window */
UnityDecoration.top {
background-image: none;
background-color: #90D0FF;
}

UnityDecoration.left {
background-image: none;
background-color: #90D0FF;
}

UnityDecoration.right {
background-image: none;
background-color: #90D0FF;
}

UnityDecoration.bottom {
background-image: none;
background-color: #90D0FF;
}


/* Black around inactive window */
UnityDecoration.top:backdrop {
background-image: none;
background-color: #000000;
}

UnityDecoration.left:backdrop {
background-image: none;
background-color: #000000;
}

UnityDecoration.right:backdrop {
background-image: none;
background-color: #000000;
}

UnityDecoration.bottom:backdrop {
background-image: none;
background-color: #000000;
}
&lt;/pre&gt;

&lt;p&gt;It removes all shadows, puts a one pixel black border around every window, and changes the color of that one pixel border to light blue for the active window. After adding this file, log out and log back in to see the change. If you don’t like it, simply delete the &lt;code class=&quot;highlighter-rouge&quot;&gt;gtk.css&lt;/code&gt; file you created, and things will be back to normal. One last note: this might fail to remove shadows, in which case you should proceed to the following step.&lt;/p&gt;

&lt;h2 id=&quot;get-rid-of-window-shadows-optional&quot;&gt;Get rid of window shadows (optional)&lt;/h2&gt;

&lt;p&gt;If the above step failed to remove shadows, then we need to tell Unity to have &lt;code class=&quot;highlighter-rouge&quot;&gt;gtk.css&lt;/code&gt; override your theme’s shadow settings. The tool we need here is &lt;em&gt;compizconfig&lt;/em&gt;. To install, type&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get update&lt;/code&gt; &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install compizconfig-settings-manager&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Compizconfig has a lot of overlap with unity tweak tool, but allows you to do even more. This power comes with some drawbacks - you can completely ruin your visual environment, and then you’ll have to use terminal to restore the default settings (not the end of the world, but quite scary).&lt;/p&gt;

&lt;p&gt;If you open compizconfig, it should look like this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/unity-can-be-beautiful/compiz.png&quot; style=&quot;width: 100%; object-fit: contain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you click &lt;strong&gt;Ubuntu Unity Plugin&lt;/strong&gt;, you’ll see a menu like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/unity-can-be-beautiful/compiz-unity.png&quot; style=&quot;width: 100%; object-fit: contain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click the &lt;strong&gt;decorations&lt;/strong&gt; tab to see this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/unity-can-be-beautiful/compiz-unity-decorations.png&quot; style=&quot;width: 100%; object-fit: contain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This tab lets us adjust window shadows, but not below the size of one pixel (annoying). To completely erase window shadows, make sure &lt;strong&gt;Override Theme Settings&lt;/strong&gt; is not checked, so that our &lt;code class=&quot;highlighter-rouge&quot;&gt;gtk.css&lt;/code&gt; file dictates the shadow settings, which we had specified as being zero pixels in size.&lt;/p&gt;

&lt;h2 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;I hope you liked this walk-through to get your Unity desktop uncluttered. Feel free to change the color of the active window border in &lt;code class=&quot;highlighter-rouge&quot;&gt;gtk.css&lt;/code&gt;, or make any other modifications. Do be careful messing around with compizconfig.&lt;/p&gt;

&lt;p&gt;One more quick Unity beautification you can do is better font rendering. Just install infinality, choose the setting called &lt;em&gt;infinality&lt;/em&gt;, and then edit one line inside of a CSS file to also reflect &lt;em&gt;infinality&lt;/em&gt;. The tutorial I followed for all that is &lt;a href=&quot;http://www.webupd8.org/2013/06/better-font-rendering-in-linux-with.html&quot;&gt;here&lt;/a&gt;. That’s it - cheers, and welcome to the most beautiful operating system of your life :)&lt;/p&gt;

</content>
		</entry>
	

</feed>



</html>
