{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# PyMLP\n",
    "\n",
    "A scratch class-based implementation of a multilayer perceptron in Python, geared for MNIST.\n",
    "\n",
    "_April 16, 2017_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 3rd party packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# My MNIST loader (pip install mnist_web)\n",
    "from mnist_web import mnist, render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mnist_numpy(onehot_labels=False, flat_images=False):\n",
    "    \"\"\"Return train_images, train_labels, test_images, test_labels.\"\"\"\n",
    "    # Load MNIST\n",
    "    train_images, train_labels, test_images, test_labels = mnist()\n",
    "\n",
    "    if flat_images:\n",
    "        train_images = np.array([np.array(img).flatten() for img in train_images])\n",
    "        train_labels = np.array(train_labels)\n",
    "        test_images = np.array([np.array(img).flatten() for img in test_images])\n",
    "        test_labels = np.array(test_labels)\n",
    "    else:\n",
    "        train_images = np.array([np.array(img) for img in train_images])\n",
    "        train_labels = np.array(train_labels)\n",
    "        test_images = np.array([np.array(img) for img in test_images])\n",
    "        test_labels = np.array(test_labels)\n",
    "\n",
    "    if onehot_labels:\n",
    "        # Cast labels to one-hot vectors\n",
    "        def cast_to_one_hot(integers):\n",
    "            \"\"\"Return matrix whose rows are one-hot vectors for integers.\"\"\"\n",
    "            output = np.zeros((integers.size, integers.max() + 1))\n",
    "            output[np.arange(integers.size), integers] = 1\n",
    "            return output\n",
    "\n",
    "        train_labels = cast_to_one_hot(train_labels)\n",
    "        test_labels = cast_to_one_hot(test_labels)\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_images, train_labels, test_images, test_labels = mnist_numpy(onehot_labels=True, flat_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot(function):\n",
    "    xs = np.arange(-5, 5, 0.01)\n",
    "    ys = [function(x) for x in xs]\n",
    "    dys = [function(x, deriv=True) for x in xs]\n",
    "\n",
    "    plt.plot(xs, ys)\n",
    "    plt.plot(xs, dys)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def linear(x, labels=None, deriv=False):\n",
    "    \"\"\"Linear layer.\"\"\"\n",
    "    if deriv:\n",
    "        return np.ones_like(x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24ff8318fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Linear')\n",
    "plot(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, labels=None, deriv=False):\n",
    "    \"\"\"Numerically-stable sigmoid.\"\"\"\n",
    "    if deriv:\n",
    "        z = sigmoid(x)\n",
    "        return z * (1 - z)\n",
    "    else:    \n",
    "        if np.all(x) >= 0:\n",
    "            z = np.exp(-x)\n",
    "            return 1 / (1 + z)\n",
    "        else:\n",
    "            z = np.exp(x)\n",
    "            return z / (1 + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8VfX9x/HXJztkMkIYYYQR9lAiqIAEcYClai1arDiL\ntLb8tK217tpaa1tr66hWpda6BcWFioqDWGUP2XuEEQghAbL3/fz+OMHGNJAA9+bk3vt5Ph73kdx7\nv8n5fCG8Ofme7/l+RVUxxhgTWELcLsAYY4z3WbgbY0wAsnA3xpgAZOFujDEByMLdGGMCkIW7McYE\nIAt3E3BE5CoRmdfSjisimSIytTlrMsHLwt34LREZJSILRaRARA6JyAIROUNVX1HVC5q7HreOa0xD\nwtwuwJiTISLxwPvATcDrQAQwGqhwsy5jWgo7czf+Kg1AVV9T1RpVLVPVeaq6RkSuE5GvjjYUkQtE\nZHPtGf4/ROSLo8MjtW0XiMgjInJERHaIyNm1r+8RkVwRubbO90oQkRdF5KCI7BKRe0QkpM73qnvc\n80VkU+1xnwCk2f50TNCzcDf+agtQIyIviMgEEWndUCMRaQfMBu4E2gKbgbPrNRsBrKl9/1VgJnAG\n0AuYAjwhIrG1bf8OJAA9gDHANcD1xzjuW8A9QDtgOzDyZDtrzImycDd+SVULgVGAAv8EDorIHBFJ\nrtf0ImC9qr6lqtXA40BOvTY7VfXfqloDzAK6APeraoWqzgMqgV4iEgpMBu5U1SJVzQL+ClzdQIlH\njztbVauARxs4rjE+Y+Fu/JaqblTV61Q1BRgIdMIJ0bo6AXvqfI0Ce+u1OVDn87LadvVfi8U5Aw8H\ndtV5bxfQuYHyGjrungbaGeMTFu4mIKjqJuB5nJCvaz+QcvSJiEjd5ycoD6gCutV5rSuQ3UDb/Ti/\nAdQ9bpcG2hnjExbuxi+JSF8RuVVEUmqfdwGuBBbXa/oBMEhELhWRMOBnQIeTOWbtsM3rwB9EJE5E\nugG/BF5uoPkHwAARuaz2uDef7HGNORkW7sZfFeFcCF0iIiU4ob4OuLVuI1XNAy4HHgLygf7Ack5+\nyuT/ASXADuArnAuwz9VvVOe4f6o9bm9gwUke05gTJrZZhwkmtdMW9wJXqep8t+sxxlfszN0EPBG5\nUEQSRSQSuAtnvnn94RtjAoqFuwkGZ+HMM88Dvgtcqqpl7pZkjG/ZsIwxxgQgO3M3xpgA5NrCYe3a\ntdPu3bu7dfiTVlJSQkxMjNtlNCvrc3AItj77a39XrFiRp6pJjbVzLdy7d+/O8uXL3Tr8ScvMzCQj\nI8PtMpqV9Tk4BFuf/bW/IrKr8VY2LGOMMQHJwt0YYwKQhbsxxgQgC3djjAlAjYa7iDxXuxvNumO8\nLyLyuIhsE5E1InK698s0xhhzIppy5v48MP4470/AWRSpNzANeOrUyzLGGHMqGg13Vf0PcOg4TS4B\nXlTHYiBRRDp6q0BjjDEnzhvz3Dvz7R1m9ta+tr9+QxGZhnN2T3JyMpmZmV44fPMqLi72y7pPhfU5\nOARbn33R32qPUloFJVVKWY1SXg0VNUpZNZRXKxU1zseh7UNJTQj16rHra9abmFR1BjADID09Xf3x\nBgJ/vfHhVFifg0Ow9fl4/VVViiqqyS+uJK+4gvziCg4WV5JfXMGR0ioKy6ooKKuisNz5WFBWRWFZ\nNWVVNU069hmD+pBxZrfGG54Cb4R7Nt/ePiyFhrcdM8YY11VWezhQWM7mQzUc+TqbfQVl7D9Szv6C\nMnKLKsgrqiCvpJLKak+DXx8XFUZCdDjxUeEkRIeT2i7mW8/jo52PcVFhtIoIIzYyjFaRocRGhhET\nGUar8FBCQsTn/fRGuM8BpovITJydcQpU9X+GZIwxprkUV1SzK7+ErLxSsvJLyMorISu/hF35pRws\nruCbxXCXrgIgPiqMTonRJMdH0bt9HO1iI2gXG0nbOh+TYiNpHRNBeKh/zCBvNNxF5DUgA2gnInuB\n+3B2gEdVnwbmAhcB24BS4HpfFWuMMXWVVdawNbeITfuL2JRTxOYDhWw5UMzBom/vopgUF0lq2xjG\npCXRKTGaTolRHNy1hfHnjKBjQjQxka4ts+UzjfZIVa9s5H3F2XTYGGN8pqyyhnX7Cli1+wir9h5h\nw75CsvJLvjkLjwwLIS05jnN6J9EjKYbubWPo3q4V3dvGNBjemSU76NU+rpl70XwC778rY0xAyCko\nZ9GOPJZlHWbV7iNsPlBEjcdJ8s6J0QzqnMDFQzrRt0McfTrE0a1tDKHNMJbtLyzcjTEtQm5hOYt2\n5LN4Rz6LtueTlV8KOBcwh3ZJ5Ka+PRnaJZEhXRJJiot0udqWz8LdGOMKj0dZm13AZ5tymb8pl7XZ\nBYAT5iNS2zLlzG6c1bMt/TrEN8vskkBj4W6MaTZVNR4Wbs9n7pr9fLYpl7ziCkIETu/amtsu7MOY\ntCT6dYy34RUvsHA3xvhUjUdZlnWI91bv48N1ORwqqSQuMoyMvu0Z17c9Y9KSaB0T4XaZAcfC3Rjj\nE3sOlTJr2R5mr9hLTmE50eGhnNc/me8O7siYPklEhvn29vtgZ+FujPGaymoP8zbkMGvZHr7cmkeI\nwJi0JO6Z2I9z+7anVYRFTnOxP2ljzCk7XFLJq0t388LCLHKLKuicGM0vzkvj8vQUOiVGu11eULJw\nN8actJ15Jfzrqx3MXrGX8ioP56Ql8edJ3Tmnd5JdFHWZhbsx5oTtOFjM3z/fxrursgkLCeF7p3Xm\nhlGp9OkQuHd8+hsLd2NMk+3MK+Hvn23lnVXZRISFMHV0D6aOTqV9XJTbpZl6LNyNMY3KL67g0U+3\n8urS3YSHCj8alcq0c3ranaItmIW7MeaYKqpreH5BFk98vo3Sqhp+OLwrN4/rbaHuByzcjTEN+nTD\nAX73/nr2HCrj3L7tueuivgG9imKgsXA3xnxLTkE5v52zno/W55CWHMtLPxrO6N5JbpdlTpCFuzEG\nAI8qLy7K4qGPNlNV4+HX4/swdVQPIsL8Y+ch820W7sYY9h4u5c9Ly9l8eD2je7fjgUsH0q1tjNtl\nmVNg4W5MEFNVZq/Yy+/e20B1tYe/TBrMpGEpiNgNSP7Owt2YIHW4pJI73lrDx+sPMDy1DZd3KePy\n9C5ul2W8xAbTjAlCX+8+zMS/f8X8TQe566K+vHbjmSS1sjgIJHbmbkwQUVVeXLSLBz7YQHJ8FLNv\nOovBKYlul2V8wMLdmCBRUlHN7W+u4f01+zmvX3v+evlQElqFu12W8RELd2OCQPaRMqa+sJzNOYXc\nMaEv00b3sH1JA5yFuzEBbuXuw0x7cQUVVTU8f/1wzkmzG5KCgYW7MQHs3VXZ3DZ7DR3io5g5bYQt\nHxBELNyNCUCqylNfbOehjzYzPLUNT08ZRhvbhDqoWLgbE2A8HuXBuRt59qudXDykEw9fPsSWEAhC\nFu7GBJCqGg+3v7mGt1Zmc93Z3fnNxP524TRIWbgbEyDKq2r46Ssr+XxTLreen8b0c3vZMgJBzMLd\nmABQXlXDjS8u56tteTxw6UCmnNnN7ZKMyyzcjfFzdYP9L5OGMGlYitslmRagSVdZRGS8iGwWkW0i\nckcD73cVkfki8rWIrBGRi7xfqjGmvrLKGqa+YMFu/lej4S4iocCTwASgP3CliPSv1+we4HVVPQ2Y\nDPzD24UaY77t6Bn7gu15PGzBbuppypn7cGCbqu5Q1UpgJnBJvTYKxNd+ngDs816Jxpj6qmo8TH91\nJQu25/HXy4fwfQt2U4+o6vEbiEwCxqvq1NrnVwMjVHV6nTYdgXlAayAGOE9VVzTwvaYB0wCSk5OH\nzZw501v9aDbFxcXExsa6XUazsj63LB5Vnl1bycJ91VzTP4Jzu3pn8a+W3Gdf8Nf+jh07doWqpjfW\nzlsXVK8EnlfVv4rIWcBLIjJQVT11G6nqDGAGQHp6umZkZHjp8M0nMzMTf6z7VFifWw5V5XfvbWDh\nvix+dUEa08/t7bXv3VL77CuB3t+mDMtkA3W3Z0mpfa2uHwGvA6jqIiAKaOeNAo0x//X4Z9t4fmEW\nU0el8rOxvdwux7RgTQn3ZUBvEUkVkQicC6Zz6rXZDYwDEJF+OOF+0JuFGhPsZi7dzSOfbmHSsBTu\n/k4/u0HJHFej4a6q1cB04GNgI86smPUicr+IXFzb7FbgRhFZDbwGXKeNDeYbY5rsy60HufuddYxJ\nS+JPlw2yYDeNatKYu6rOBebWe+03dT7fAIz0bmnGGIDNOUX89OWV9G4fyxM/PI2wUFsEzDTOfkqM\nacFyC8u54fllREeE8tx1ZxAXZdvimaax5QeMaaHKKmuY+uJyDpVU8sZPzqJTYrTbJRk/YuFuTAuk\nqtzx1hrWZhcw4+p0BnZOcLsk42dsWMaYFujZL3fy7qp93Hp+Guf3T3a7HOOHLNyNaWG+2prHHz/c\nyISBHWwuuzlpFu7GtCC780uZ/tpKereP4+HLh9iUR3PSLNyNaSFKK6uZ9tJyPB5lxjXDiIm0S2Lm\n5NlPjzEtgKpy99vr2HygiOevH063tjFul2T8nJ25G9MCvLF8L29/nc0t43ozJi3J7XJMALBwN8Zl\nm3IKuffddYzs1Zb/8+Iqjya4Wbgb46KSimp+9spK4qPDefQHpxEaYhdQjXdYuBvjElXlnnfWsTOv\nhMcmDyUpLtLtkkwAsXA3xiWvL9/D219n8/Pz0ji7p21/YLzLwt0YF2zLLea+OesZ1aud3ahkfMLC\n3ZhmVlnt4eezvqZVRBh/u2KIjbMbn7B57sY0s0c+3cK67EKeuXoY7eOj3C7HBCg7czemGS3ekc/T\nX2xn8hlduHBAB7fLMQHMwt2YZlJQVsUvZ62iW5tW3Duxv9vlmABnwzLGNJN731nHgaIK3rzpbFs3\nxvicnbkb0wzeXZXNnNX7uGVcb4Z2SXS7HBMELNyN8bGcgnLueWcdp3dN5KcZPd0uxwQJC3djfEhV\nuevttVTVePjrFUMJC7V/cqZ52E+aMT705spsPt+Uy20X9iW1nS3ja5qPhbsxPpJTUM7v3lvPGd1b\nc/3Z3d0uxwQZC3djfKDucMxDk4YQYnehmmZm4W6MD9hwjHGbhbsxXmbDMaYlsHA3xovqDsf8xYZj\njIss3I3xondX7ePzTbn8+sK+dLfhGOMiC3djvKSgtIoHPtjAkC6JXGvDMcZlFu7GeMmfP97EoZJK\nHvzeQFuj3biuSeEuIuNFZLOIbBORO47R5goR2SAi60XkVe+WaUzLtmLXYV5dspvrR6YyoFOC2+UY\n0/iqkCISCjwJnA/sBZaJyBxV3VCnTW/gTmCkqh4Wkfa+KtiYlqa6xsPdb6+lY0IUvzg/ze1yjAGa\nduY+HNimqjtUtRKYCVxSr82NwJOqehhAVXO9W6YxLde/F2SxKaeI+747gFhbyte0EE35SewM7Knz\nfC8wol6bNAARWQCEAr9V1Y/qfyMRmQZMA0hOTiYzM/MkSnZXcXGxX9Z9KqzPx5Zf5uHhr8oYkhRK\n5MGNZGZu8n1xPhJsf8+B3l9vnWaEAb2BDCAF+I+IDFLVI3UbqeoMYAZAenq6ZmRkeOnwzSczMxN/\nrPtUWJ+PbdqLywkJqeTJG84hpXUr3xfmQ8H29xzo/W3KsEw20KXO85Ta1+raC8xR1SpV3QlswQl7\nYwLWJxsOMG/DAW45r7ffB7sJPE0J92VAbxFJFZEIYDIwp16bd3DO2hGRdjjDNDu8WKcxLUpJRTX3\nvbuOPslx/GhUqtvlGPM/Gh2WUdVqEZkOfIwznv6cqq4XkfuB5ao6p/a9C0RkA1AD3Kaq+b4s3Bg3\nPf7ZVvYVlDP7ytMItw04TAvUpDF3VZ0LzK332m/qfK7AL2sfxgS0jfsLefarnUw+owvp3du4XY4x\nDbJTDmNOgMej3P32WhKiw7l9fF+3yzHmmCzcjTkBs5bvYeXuI9x9UT9ax0S4XY4xx2ThbkwT5RVX\n8KcPNzEitQ2Xnd7Z7XKMOS4Ld2Oa6MEPNlJaWc0fvjcIEVsYzLRsFu7GNMHC7Xm89XU2PxnTk17t\nY90ux5hGWbgb04iK6hrueXsdXdu04mdje7ldjjFNYqscGdOIZ77YwY68El64YThR4aFul2NMk9iZ\nuzHHkZVXwhPztzFxcEfGpCW5XY4xTWbhbswxqCr3vruOyNAQ7p3Y3+1yjDkhFu7GHMN7a/bz5dY8\nfnVhH5Ljo9wux5gTYuFuTANKqpTfv7+BwSkJTDmzm9vlGHPC7IKqMQ14c2sl+cXVPHftGbbZtfFL\nduZuTD2r9hxh/u5qrjmrO4NSbLNr458s3I2p4+hm1wmRwq0X2GbXxn9ZuBtTxwuLdrF+XyFX9Ysg\nLirc7XKMOWkW7sbU2l9Qxt/mbSajTxLpyXazkvFvFu7G1Lr/vQ1Ue5TfXzLQFgYzfs/C3Rjg800H\n+HBdDjeP602XNrbZtfF/Fu4m6JVV1nDvO+vp3T6WG0f3cLscY7zC5rmboPf451vJPlLGrGlnEhFm\n5zsmMNhPsglqm3OK+Od/dnD5sBRG9GjrdjnGeI2FuwlaHo9yzztriYsK486L+rldjjFeZeFugtbs\nFXtZlnWYOy/qRxvb7NoEGAt3E5Tyiyt48MONDO/ehkmnp7hdjjFeZ+FugtIfP9xEcXk1D3xvICG2\nMJgJQBbuJugs2p7P7BV7ufGcHqQlx7ldjjE+YeFugkpFdQ13v72Wrm1acfO5vd0uxxifsXnuJqg8\nlbn9m82uoyNs/RgTuOzM3QSNbbnF/GP+di4e0sk2uzYBz8LdBAVV5e631xIVHsI9E21Ouwl8Fu4m\nKMxesZclOw9xx4R+tI+zza5N4GtSuIvIeBHZLCLbROSO47T7voioiKR7r0RjTs2hkkoenLuR9G6t\nmXxGF7fLMaZZNBruIhIKPAlMAPoDV4pI/wbaxQG3AEu8XaQxp+IPH2ykqLyaBy8bZHPaTdBoypn7\ncGCbqu5Q1UpgJnBJA+1+D/wZKPdifcackoXb83hz5V5+PMbmtJvg0pSpkJ2BPXWe7wVG1G0gIqcD\nXVT1AxG57VjfSESmAdMAkpOTyczMPOGC3VZcXOyXdZ8Kf+1zZY1y74IykqKFIWH7yczMafLX+muf\nT0Ww9TnQ+3vK89xFJAT4G3BdY21VdQYwAyA9PV0zMjJO9fDNLjMzE3+s+1T4a5//Nm8zB0q38eIN\nwznnBKc++mufT0Ww9TnQ+9uUYZlsoO5VqJTa146KAwYCmSKSBZwJzLGLqsZNG/YV8o/M7XzvtM4n\nHOzGBIKmhPsyoLeIpIpIBDAZmHP0TVUtUNV2qtpdVbsDi4GLVXW5Tyo2phHVNR5uf3MNia3C+c3E\n/7n2b0xQaDTcVbUamA58DGwEXlfV9SJyv4hc7OsCjTlR//xyJ2uzC7j/koG0tnXaTZBq0pi7qs4F\n5tZ77TfHaJtx6mUZc3K2HyzmkU+3MH5ABy4a1NHtcoxxjd2hagKGx6Pc8eYaosNDuf/SAW6XY4yr\nLNxNwHhp8S6WZR3m3on9bYkBE/Qs3E1A2HOolD9/tIkxaUl8//TObpdjjOss3I3fU1XuenstAjx4\n2SBEbIkBYyzcjd97deluvtyaxx0X9aNzYrTb5RjTIli4G7+WlVfCA+9vZHTvdkwZ0dXtcoxpMSzc\njd+q8Si/emM1YaHCQ5MG23CMMXXYHqrGb/3zyx0s33WYR34whI4JNhxjTF125m780qacQv42bwsT\nBnbg0qE2O8aY+izcjd+prPbwi1mriY8O54FLB9pwjDENsGEZ43ce+2wLG/cX8uw16bSNjXS7HGNa\nJDtzN35l8Y58nsrczhXpKZzXP9ntcoxpsSzcjd84XFLJL2atolvbGO77rq0dY8zx2LCM8Quqyu1v\nriGvuIK3fzqSmEj70TXmeOzM3fiFl5fsZt6GA9w+vi8DOye4XY4xLZ6Fu2nxNuUU8vv3NzAmLYkb\nRqa6XY4xfsHC3bRoZZU13Pza18RHhfPw5UMICbFpj8Y0hQ1cmhbtt3PWs+VAMS/eMJykOJv2aExT\n2Zm7abFmLdvNrOV7+NnYnpyTluR2Ocb4FQt30yKtyy7g3nfXM7JXW355fh+3yzHG71i4mxanoLSK\nm15ZQduYCB6ffBqhNs5uzAmzMXfTong8yi9fX0VOQTmzfnyWLS9gzEmyM3fTovwjcxufbcrl3on9\nOb1ra7fLMcZv2Zm7aTHmrc/hr59s4dKhnbj6zG6+P2DpIcjfDod2QGkelB2B8gJA6ZW9Hyo/g1Zt\nISYJ4jtCuz4Q3wlsFUrjByzcTYuwKaeQn89axeDOCfzp+z7YVam6EvYsgT2LYc8yyF4Opfn/2y4y\nHiSEDlWVkFMDNRXffj8iDtr3g65nQreznY/R9huGaXks3I3r8osrmPrCcuKiwphxTTpR4aHe+cbl\nBbDpA9j8IWyfD5VFzuvt+kDaBCek2/aCNj0gNskJ9hDn2F9lZpKRkQGVJVByEI7sgbzNcHAL7F8N\nS56GhY+DhEDXs6Hvd5xH62b4jcOYJrBwN66qrPZw08srOVhUwes/Povk+KhT+4aeGifIV7/qBHt1\nOcR1hEHfh94XQrezTuxMOyLGebTuDqmj//t6VRlkr3COtXkufHyn8+g2Ck6bAv0vdr7OGJdYuBvX\nqCr3vLOWpVmHeGzyUIZ0STz5b1ZeACtfgqXPwJHdEJXohOyQK6HzMO+Pk4dHQ/dRzmPcvc64/bq3\nYNUr8M5PYO5tMPRKOPMm5zcDY5qZhbtxzWOfbeX15Xu5+dxeXHKy+6Ae2QML/+6EamUxdBsJ5/8e\n+kyAsGacRtmmB5zzKxh9K+xeBCtegOX/hmXPQt+JcPbN0OWM5qvHBD0Ld+OKWct28+inW5k0LIVf\nnJ924t+gIBu+/CusfNF5PmgSjPgJdBrq3UJPlIhzobXb2XDeb2HpDFj+HGycAz3PhbF3Q0q6uzWa\noGDhbprd/E253PX2Okb3bscfLxt0YjNjinLgPw/DyhdA1Rl6GX0rJHbxXcEnK74jnHefU9/y52DB\no/DsOGfsf+xd7v9HZAJak25iEpHxIrJZRLaJyB0NvP9LEdkgImtE5DMRsSkDpkFr9h7hp6+spG+H\nOJ6aMozw0CbeR1dZCl88BI+fDiv+DUN/CDevhO8+2jKDva7IWBh5M9yyGs6915mSOWMMvDXN+Q3E\nGB9o9F+WiIQCTwITgP7AlSLSv16zr4F0VR0MzAYe8nahxv9tOVDEtc8tpU1MBP++7gxim7JVnscD\nq2fC34fB/D9Ar3EwfRl89zFI7Or7or0pMs4Zl//5Ghj1C1j/Tm2/HnSmXBrjRU05bRoObFPVHapa\nCcwELqnbQFXnq2pp7dPFQIp3yzT+bmdeCVc9u4Tw0BBevXEE7Zsy5XHvcnj2XHj7xxCXDNd/CD94\nyf9nn0QlOOPx05dBn/HwxZ+dkF/1mjPUZIwXiDbywyQik4Dxqjq19vnVwAhVnX6M9k8AOar6QAPv\nTQOmASQnJw+bOXPmKZbf/IqLi4mNjXW7jGZ1qn3OK/Pw4JJyqmqUO0ZE0zn2+OcUYVXFpO58mU77\nPqIyojU7elzLgeRznBuGmklz/j3HF2yk17Z/EV+0lSMJ/dmSdhOlMc3/W0mw/Wz7a3/Hjh27QlUb\nvyqvqsd9AJOAZ+s8vxp44hhtp+CcuUc29n2HDRum/mj+/Plul9DsTqXPBwrKdMxDn+ug+z7StXuP\nHL+xx6O65g3Vh3qp/jZRde7tquWFJ33sU9Hsf881NaorXlD9UzfV37VR/fge1fKiZi0h2H62/bW/\nwHJtJF9VtUmzZbKBulesUmpf+xYROQ+4GxijqhX13zfBZ39BGVf9cwkHiyp4aeoIBnZOOHbjQzvg\ng1th++fQ6TS46o3gmk0SEgKnXwN9vgOf/sZZ2mDdWzDhT848eVuszJygpvyeuwzoLSKpIhIBTAbm\n1G0gIqcBzwAXq2qu98s0/mbPoVKueGYRuUUVPH/D8GMv31tdCf/5C/zjLGdBrwl/gamfBVew1xXT\nFi55Em742BmbnzUFXv0BHM5yuzLjZxoNd1WtBqYDHwMbgddVdb2I3C8iF9c2+wsQC7whIqtEZM4x\nvp0JAjvzSvjBM4soKK3i5akjOKN7m4YbZn0FT4+Czx+AtPHOBcYR075ZvCuodT0TfvwFXPAH58/p\nyTPhy785/xka0wRNuolJVecCc+u99ps6n5/n5bqMn9p6oIirnl1CtUd5bdqZDOjUwFBMST58cq+z\nZEBiV/jhG5B2QfMX29KFhsPZ02HApfDh7fDZ72DN6zDxEWcBNGOOw3ZiMl6zLOsQk55ehEdhZkPB\nrgpfvwxPpMOaWc5c758usWBvTEIKTH4FrpzprJ/z7/Hw7nRnsxFjjsGWHzBe8eHa/dwyaxUpidG8\ncMNwurRp9e0GBzfD+7+AXQugywiY+Cgk178XzhxXnwmQeg5k/gkWPeksNXzBA87Kl3bB1dRjZ+7m\nlD331U5++upKBnaKZ/ZNZ3872KvK4LPfw1Mj4cB6587S6z+yYD9ZETFwwe/hx/+BNj3hnZvg+YnO\nJiLG1GFn7uakVdV4+MMHG3l+YRYXDkjmscmnfXsXpa2fwtxbnZkeg3/gXByMTXKt3oDSYaAzo2bl\nC/DpffDU2TDq584iZeHRbldnWgALd3NSDpVU8rNXVrJoRz43jEzl7u/0IzSkdmigcB98dCdseAfa\n9oZr33OGE4x3hYRA+vXO9n7z7nGmlK6dDd/5q7MGjwlqFu7mhG3YV8i0l5aTW1TBw5cPYdKw2qWE\naqqdzSk+fwA8VTD2Hmc1xObcNCMYxbaHy2Y4K2W+/0t4+TIY+H248I/OmjwmKFm4mxPy7qps7nhz\nLfHRYbz+47MYenRrvOwVzgXT/auh5zj4zsP+v8CXv+mRATcthK8ega/+5gyLjbsX0m+weweCkF1Q\nNU1SWlnNr2ev5paZqxjQKZ73po9ygr3oALz7M/jnOOfzy5+HKW9asLslPArG3gk3LXLu8p37K/jX\n+c5/uiao2Jm7adSeIg8PPLGA7QeLmT62Fz8/rzdhWg0LHnc20Kgud262Oec255Z54752veCad2Ht\nG/DxXTCk0Rk1AAAM7klEQVQjwzmDz7jLWeLABDwLd3NMHo/ywqIsHlxURmJMJC//aAQje7aFrfOc\nC6aHtjtbxl34oBMmpmURgcFXQO/znesgy/8Na95wNgwZ8WO7FhLgbFjGNGhXfgmT/7mY3723gX5t\nQ5l782hGxubAK5Pg1SuctdWvmg1XvW7B3tJFt3Zm0Ny0ELoMd5Z+eHIEbJhjm4MEMDtzN9/i8Sgv\nLsrizx9tJixEeGjSYLocXEDSJ//nrGsSFe+cqQ+f5qx9YvxH+74wZTZs+xQ+vgdevxq6jYTz74eU\nxvd+MP7Fwt18Y+3eAu59dx2r9hxhTFoSD03oSPKqJ/Es/SeEhsHIW5wbZaKPsXyv8Q+9zoPUDPj6\nRfj8D/DsOEibQEz8hUCGy8UZb7FwNxSUVvHwvM28vGQXbWMi+Pul3ZhY+g7y76ehqoScDuPoNPkx\nSOjsdqnGW0LDnAusgy6HJU/Dgr9zRsWHUPYfyLgTktLcrtCcIgv3IFZV42HWsj088skWDpdWclN6\nAjfHzCPq8385qw/2uxjOvZct6/fRyYI9MEXGObOczpjKrtduo9uWuc6dxYMnO6t2Wsj7LQv3IKSq\nfLguh798vJmdeSWc3wX+OCCTdhtfdhb6GvA9Z0ZF8oDar9jnar2mGUS3ZmePKXS74o/OTVDLn4PV\nr0G/iU7Idx7mdoXmBFm4BxFVZeH2fB76eDOr9xzhgnZ5vNLvCzrufg/Jq3Z+RR99KyT1cbtU45bY\nJBj/IIz+pTNcs3QGbHwPUsc4Id8jw5YX9hMW7kFAVflsYy5PzN/G6j2HuDR2E8+kfEqHvMVQ0crZ\nmPnMn0Lbnm6XalqKmHZw7j1w9s2w4nln/fiXLoX2/WH4jc4qnxExbldpjsPCPYBV1XiYu3Y/T2Vu\n50BONjfELuaFtpkklGRBRUcYdx8Muw5aHWOPU2Oi4p3F34ZPg3WzYckzzhpCn/wWTpsCw6faUhMt\nlIV7AMotKue1JXt4ZXEWPUu/5tet/sOYVosJra6CDsPhgruccfWwCLdLNf4iPMoJ86FXwZ6lsPQZ\n57H4H85yzqdNgb4TIaJV49/LNAsL9wDh8SjLsg7x2tLdrFu7ku+wgDnRi+gQsQ8NS0CG/AhOv9Z2\nQDKnRgS6jnAeRTmw4gVno/O3boTIeBh4GQyd4twUZWPzrrJw93O780t5c+VeMleuZVjhfH4UtpBB\n4dtRBEkZBafdh/S/xHbnMd4X1wEybnemUu5a4IT86lnOGH3rVBhwqfMbYofBFvQusHD3QwcKy5m3\nPoelK1eQvO8zLgxdzi0hWwkJ9+BJHgxDHkAGXGY3HZnmERICqaOdx4SHnHny6992Vg396pHaoP8e\n9L8EOg6xoG8mFu5+Ys+hUj5es5ftq78k5eAXnB+ynKtDsiEcqtoNIGTAbTDw+4TYNEbjpqh4Z/bV\n6ddAST5ser826B9zNhCJ7eCsUpl2oTOtMjLO7YoDloV7C1VWWcPiHXmsX7sCz7b59C1dwRUhG4iX\nMjxhoZR3GgGDb4Y+Ewhv3c3tco35XzFtYdi1zqMkH7Z8BFs/hg3vwtcvQUg4dDsbeo6F7qOh41Bn\nWQTjFfYn2UKUV9Wwelc+OzYsp2zHItoc+pozZANjJR+AophOSM/LoP/5hKSOoZVNXzT+JKYtnHaV\n86ipgt2LnX0Btn4Cn/7WaRMRC13Pgu6jnEeHwTaj6xRYuLtAVcktqmDTtu3kbFqCJ3sFnYvWMFS2\nMkLKACiOaENZxzOoGnAB4WnnEmdziU2gCA3/7xj9Bb+H4lzI+uq/j0/vq20XCR0HQ+d0Z/mDlGHO\n+L2N2TeJhbuPqSp784vZsW0TR3auQHLW0LpwE2me7YyRIwB4EPJielDY8RJC+40mpufZxLZOJdZ+\niE0wiG3vTKEceJnzvOgA7F4Ie5c7G6+veB6WPOW8F90GOgyE5IHO2kft+0NSX5tf3wALdy+prvGw\nJzef/TvWU5y9Ac3dTHThDpIrsujGfrpIFQA1hJAb2Y2iNqOoSBlKhz7DiUgZSnvbe9QYR1yyM7tm\nwPec5zXVkLvBCfrsFXBgvbNlYLXzWy4S4twl274ftO0FbXo6H9v2hJikoD3Tt3BvIlUl70gRuQf2\nsnDeG1Tk7UQP7yaqeA/xFftpX3OAVDlCam17D8LB0A4UJKSyo00GsZ37k5yWTmSngXS0OefGNF1o\nmDM803EwpF/vvOapgcNZcGAdHNgAueshdyNs/hA81f/92og4aNvDCf+ELrWPFEhIIayq0NlmMEDD\nP+jDvbyikoLDeRQfzqXwUA5l+XupKdgPRTmEleYSXXGQ+Oo8WnsOkyTFXFHna6sJIS+kPQVRnciJ\nGU1um+7EdOxDUo9BxHbsS3J4FMmu9cyYABYS6pyZt+3pzJ8/qqYaCnZD/g7I3+Zs4p6/Hfavhk0f\nQE3lN01HASxt5YR9fCeITXbO9GPbQ0x7Z4XMmPbO81bt/G4mT5OqFZHxwGNAKPCsqv6p3vuRwIvA\nMCAf+IGqZnm31IZVV1VRUlxIadFhykoKqCg5QmVJIdWlhVSXF1JTXoyWHkbKDxNafpjIqgKiqgqI\n8RQS5ykknlKSRf8nhKsI5ZC0oSi8LSUx3TjSajg74zqQVwb90sfQrksa0W260CE0jA7N0VFjTONC\nw5yz9DY9oPd5337P44HSPCjYAwXZbFuRSa+kKOd54T44tAOKD/53uOdbBKITISrR2Wbym8/rf2zt\nzPWPiIPIWGcG0NGPIaHN8kdwVKPhLiKhwJPA+cBeYJmIzFHVDXWa/Qg4rKq9RGQy8GfgB74oeOmb\nj9Jh3TO00lKitYwYqSABaGzEuoQoiiSektB4ysMTyItIIScyEVq1QVq1ISy2Ha0Sk4hP6kpichei\n4tqRHBLyP6GfmZlJl9MzfNE1Y4wvhYQ4Z+Gx7aHzMPbmxtMrI+PbbVSdXciKc6HkYO3HXCf0S/Og\n7AiUH3E+HtkNZYedz7Wm8eOHRf836MfeDYMv90k3vzlcE9oMB7ap6g4AEZkJXALUDfdLgN/Wfj4b\neEJERFXVi7UCEBHfnrzYPlSHx6IRsWhEHBIZR2hUHKHR8YRFxxPRKp7I2ESiYhNoFZNITEJrYsIj\nsdWnjTHHJeLcNRsZ1/T9DY7+h3A06CsKoaLYea2iyPlYWfLfzyuKnfXyfUway18RmQSMV9Wptc+v\nBkao6vQ6bdbVttlb+3x7bZu8et9rGjANIDk5edjMmTO92ZdmUVxcTGxsrNtlNCvrc3AItj77a3/H\njh27QlXTG2vXrFcIVHUGMAMgPT1dM+r/SuQHMjMz8ce6T4X1OTgEW58Dvb8hTWiTDXSp8zyl9rUG\n24hIGM4QeL43CjTGGHPimhLuy4DeIpIqIhHAZGBOvTZzgGtrP58EfO6L8XZjjDFN0+iwjKpWi8h0\n4GOcqZDPqep6EbkfWK6qc4B/AS+JyDbgEM5/AMYYY1zSpDF3VZ0LzK332m/qfF4O+HZejzHGmCZr\nyrCMMcYYP2PhbowxAcjC3RhjAlCjNzH57MAiB4Fdrhz81LQD8hptFVisz8Eh2Prsr/3tpqpJjTVy\nLdz9lYgsb8rdYYHE+hwcgq3Pgd5fG5YxxpgAZOFujDEByML9xM1wuwAXWJ+DQ7D1OaD7a2PuxhgT\ngOzM3RhjApCFuzHGBCAL91MgIreKiIqI77dVcZGI/EVENonIGhF5W0QS3a7JV0RkvIhsFpFtInKH\n2/X4moh0EZH5IrJBRNaLyC1u19RcRCRURL4WkffdrsUXLNxPkoh0AS4AdrtdSzP4BBioqoOBLcCd\nLtfjE3X2C54A9AeuFJH+7lblc9XAraraHzgT+FkQ9PmoW4CNbhfhKxbuJ+8R4NdAwF+RVtV5qlpd\n+3QxzoYtgeib/YJVtRI4ul9wwFLV/aq6svbzIpyw6+xuVb4nIinAd4Bn3a7FVyzcT4KIXAJkq+pq\nt2txwQ3Ah24X4SOdgT11nu8lCILuKBHpDpwGLHG3kmbxKM7JmcftQnylWfdQ9Sci8inQoYG37gbu\nwhmSCRjH66+qvlvb5m6cX+Nfac7ajO+JSCzwJvBzVS10ux5fEpGJQK6qrhCRDLfr8RUL92NQ1fMa\nel1EBgGpwGoRAWeIYqWIDFfVnGYs0auO1d+jROQ6YCIwLoC3UGzKfsEBR0TCcYL9FVV9y+16msFI\n4GIRuQiIAuJF5GVVneJyXV5lNzGdIhHJAtJV1R9Xl2sSERkP/A0Yo6oH3a7HV2o3d98CjMMJ9WXA\nD1V1vauF+ZA4ZygvAIdU9edu19Pcas/cf6WqE92uxdtszN00xRNAHPCJiKwSkafdLsgXai8aH90v\neCPweiAHe62RwNXAubV/t6tqz2iNn7Mzd2OMCUB25m6MMQHIwt0YYwKQhbsxxgQgC3djjAlAFu7G\nGBOALNyNMSYAWbgbY0wA+n/B8YzADDI2EQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24f84fe8e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Sigmoid')\n",
    "plot(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperbolic tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tanh(x, labels=None, deriv=False):\n",
    "    \"\"\"Hyperbolic tangent.\"\"\"\n",
    "    if deriv:\n",
    "        return 1 - np.tanh(x)**2\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEICAYAAABbOlNNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24ff836feb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Hyperbolic tangent')\n",
    "plot(tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Rectified linear unit (ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def relu(x, labels=None, deriv=False):\n",
    "    \"\"\"Rectified linear unit.\"\"\"\n",
    "    if deriv:\n",
    "        return (x > 0) * 1\n",
    "    else:\n",
    "        try:\n",
    "            return np.maximum(x, 0, x)\n",
    "        except TypeError:\n",
    "            \"\"\"When x is not iterable.\"\"\"\n",
    "            return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCRh1LhFRBJGjVtEJGHUuEVEEuZ/AYvfyXBjAEkeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24f819c9c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Rectified linear unit')\n",
    "plot(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Softmax\n",
    "\n",
    "---\n",
    "\n",
    "Softmax is a vector-to-vector transformation that turns an unnormalized vector\n",
    "\n",
    "$$x = \\begin{bmatrix} x_0,\\ x_1,\\ \\dots,\\ x_9 \\end{bmatrix}$$\n",
    "\n",
    "into a normalized vector\n",
    "\n",
    "$$s(x) = \\begin{bmatrix} s(x)_0,\\ s(x)_1,\\ \\dots,\\ s(x)_9 \\end{bmatrix}.$$\n",
    "\n",
    "The transformation is best described element-wise. So the $i$th output is given by\n",
    "\n",
    "$$ s(x)_i = \\frac{e^{x_i}}{\\sum_{j=0}^{9} e^{x_j}}. $$\n",
    "\n",
    "### Invariance to scaling\n",
    "Softmax is invariant to additively scaling every element of $x$ by the same constant $c$.\n",
    "\n",
    "*Proof:*\n",
    "\n",
    "\\begin{align*}\n",
    "s(x + c)_i &= \\frac{e^{x_i + c}}{\\sum_{j=0}^{9} e^{x_j + c}}  \\\\\n",
    "           &= \\frac{e^{x_i}e^c}{\\sum_{j=0}^{9} e^{x_j}e^c}    \\\\\n",
    "           &= \\frac{e^c e^{x_i}}{e^c \\sum_{j=0}^{9} e^{x_j}}   \\\\\n",
    "           &= \\frac{e^{x_i}}{\\sum_{j=0}^{9} e^{x_j}} = s(x)_i\n",
    "\\end{align*}\n",
    "\n",
    "That means we can protect softmax from numerical overflow by subtracting the maximum entry of $x$ from every element of $x$\n",
    "\n",
    "$$ s(x - \\max(x))_i = \\frac{e^{x_i - \\max(x)}}{\\sum_{j=0}^{9} e^{x_j - \\max(x)}} = \\frac{e^{x_i}}{\\sum_{j=0}^{9} e^{x_j}} = s(x)_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax_matrix(matrix):\n",
    "    \"\"\"Return the row-wise softmax of matrix.\n",
    "\n",
    "    :type matrix: ndarray\n",
    "    :param matrix: unnormalized matrix with a row per example\n",
    "    \"\"\"\n",
    "    def softmax_vector(vector):\n",
    "        \"\"\"Return the softmax of vector.\n",
    "\n",
    "        :type vector: ndarray\n",
    "        :param vector: unnormalized vector\n",
    "        \"\"\"\n",
    "        row_sum = np.exp(vector).sum()\n",
    "        return np.array([np.exp(x_i) / row_sum for x_i in vector])\n",
    "\n",
    "    # Subtract the max of each row from each row (for stability)\n",
    "    row_maxes = np.max(matrix, axis=1)\n",
    "    row_maxes = row_maxes[:, np.newaxis]  # for broadcasting\n",
    "    matrix = matrix - row_maxes\n",
    "    \n",
    "    return np.array([softmax_vector(row) for row in matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check our softmax against TensorFlow's softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-3c597fe3eaa6>:6: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037045],\n",
       "       [ 0.01203764,  0.08894681,  0.24178252,  0.657233  ],\n",
       "       [ 0.00626879,  0.01704033,  0.04632042,  0.93037045]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "batch = np.asarray([[1,2,3,6],[2,4,5,6],[1,2,3,6]])\n",
    "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "y = tf.nn.softmax(x)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(y, feed_dict={x: batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047],\n",
       "       [ 0.01203764,  0.08894682,  0.24178252,  0.65723302],\n",
       "       [ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_matrix(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Derivative of Softmax\n",
    "\n",
    "---\n",
    "\n",
    "Since softmax is a vector-to-vector transformation, its derivative is a Jacobian matrix. The Jacobian has a row for each output element $s(x)_i$, and a column for each input element $x_j$. The entries of the Jacobian take two forms, one for the main diagonal entry, and one for every off-diagonal entry. Here we'll show how to compute row $i$ of the Jacobian, which is the gradient of output element $s(x)_i$ with respect to each of its input elements $x_j$. We first restate the input vector, output vector, and elementwise softmax formula for clarity.\n",
    "\n",
    "\\begin{align*}\n",
    "x &= \\begin{bmatrix} x_0,\\ x_1,\\ \\dots,\\ x_9 \\end{bmatrix} \\\\ \\\\\n",
    "s(x) &= \\begin{bmatrix} s(x)_0,\\ s(x)_1,\\ \\dots,\\ s(x)_9 \\end{bmatrix} \\\\ \\\\\n",
    "s(x)_i &= \\frac{e^{x_i}}{\\sum_{j=0}^{9} e^{x_j}} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "First compute the diagonal entry of row $i$ of the Jacobian, that is, compute the derivative of the $i$'th output of softmax, $s(x)_i$, with respect to its $i$'th input, $x_i$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial s(x)_i}{\\partial x_{i}} &= \\frac{\\sum_{j=0}^{9} e^{x_j} e^{x_i} - e^{x_i} e^{x_i}}{(\\sum_{j=0}^{9} e^{x_j})^2} \\\\\n",
    "&= s(x)_i - s(x)_i^2 \\qquad \\text{diagonal entry} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now compute every off-diagonal entry of row $i$ of the Jacobian, that is, compute the derivative of the $i$'th output of softmax, $s(x)_i$, with respect to its $j$'th input, $x_j$, where $j \\neq i$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial s(x)_i}{\\partial x_{j}} &= \\frac{\\sum_{j=0}^{9} e^{x_j} \\cdot 0 - e^{x_i} e^{x_j}}{(\\sum_{j=0}^{9} e^{x_j})^2} \\\\\n",
    "&= - s(x)_i s(x)_j \\qquad \\text{off-diagonal entry}\n",
    "\\end{align*}\n",
    "\n",
    "The form of the off-diagonals tells us that the Jacobian of softmax is a symmetric matrix. This is nice because symmetric matrices have great numeric and analytic properties. We expand it below. Each row is a gradient of one output element with respect to each of its input elements.\n",
    "\n",
    "\\begin{equation}\n",
    "J\\big(s(x)\\big) =\n",
    "\\begin{bmatrix}\n",
    "    \\nabla s(x)_0 \\\\\n",
    "    \\nabla s(x)_1 \\\\\n",
    "    \\nabla s(x)_2 \\\\\n",
    "    \\dots \\\\\n",
    "    \\nabla s(x)_9\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    s(x)_0 - s(x)_0^2 & -s(x)_0 s(x)_1 & -s(x)_0 s(x)_2 & \\dots  & -s(x)_0 s(x)_9 \\\\\n",
    "    -s(x)_1 s(x)_0 & s(x)_1 - s(x)_1^2 & -s(x)_1 s(x)_2 & \\dots  & -s(x)_1 s(x)_9 \\\\\n",
    "    -s(x)_2 s(x)_0 & -s(x)_2 s(x)_1    & s(x)_2 - s(x)_2^2 & \\dots & -s(x)_2 s(x)_9 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -s(x)_9 s(x)_0 & -s(x)_9 s(x)_1 & -s(x)_9 s(x)_2 & \\dots  & s(x)_9 - s(x)_9^2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Notice that we can express this matrix as\n",
    "\n",
    "$$\\text{diag}\\big(s(x)\\big) - s(x)_{10 \\times 1}^\\top s(x)_{1 \\times 10}.$$\n",
    "\n",
    "The term on the right is the outer product since we're working with row vectors. We labeled the dimensions to be explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def jacobian_tensor(softmaxed_matrix):\n",
    "    \"\"\"Return Jacobian tensor of a matrix whose rows are softmaxed.\n",
    "\n",
    "    :type softmax_matrix: ndarray\n",
    "    :param softmax_matrix: matrix whose rows are softmaxed\n",
    "    \"\"\"\n",
    "    def jacobian_matrix(softmaxed_vector):\n",
    "        \"\"\"Return Jacobian matrix of a softmax vector.\n",
    "\n",
    "        :type softmax_vector: ndarray\n",
    "        :param softmax_vector: softmax distribution over classes\n",
    "        \"\"\"\n",
    "        s = softmaxed_vector\n",
    "        return np.diag(s) - np.outer(s, s)\n",
    "\n",
    "    return np.array([jacobian_matrix(row) for row in softmaxed_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 6],\n",
       "       [2, 4, 5, 6],\n",
       "       [1, 2, 3, 6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047],\n",
       "       [ 0.01203764,  0.08894682,  0.24178252,  0.65723302],\n",
       "       [ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_matrix(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  6.22948920e-03,  -1.06822194e-04,  -2.90372829e-04,\n",
       "          -5.83229418e-03],\n",
       "        [ -1.06822194e-04,   1.67499567e-02,  -7.89315184e-04,\n",
       "          -1.58538193e-02],\n",
       "        [ -2.90372829e-04,  -7.89315184e-04,   4.41748369e-02,\n",
       "          -4.30951488e-02],\n",
       "        [ -5.83229418e-03,  -1.58538193e-02,  -4.30951488e-02,\n",
       "           6.47812623e-02]],\n",
       "\n",
       "       [[  1.18927379e-02,  -1.07071001e-03,  -2.91049156e-03,\n",
       "          -7.91153631e-03],\n",
       "        [ -1.07071001e-03,   8.10352810e-02,  -2.15057854e-02,\n",
       "          -5.84587856e-02],\n",
       "        [ -2.91049156e-03,  -2.15057854e-02,   1.83323732e-01,\n",
       "          -1.58907455e-01],\n",
       "        [ -7.91153631e-03,  -5.84587856e-02,  -1.58907455e-01,\n",
       "           2.25277777e-01]],\n",
       "\n",
       "       [[  6.22948920e-03,  -1.06822194e-04,  -2.90372829e-04,\n",
       "          -5.83229418e-03],\n",
       "        [ -1.06822194e-04,   1.67499567e-02,  -7.89315184e-04,\n",
       "          -1.58538193e-02],\n",
       "        [ -2.90372829e-04,  -7.89315184e-04,   4.41748369e-02,\n",
       "          -4.30951488e-02],\n",
       "        [ -5.83229418e-03,  -1.58538193e-02,  -4.30951488e-02,\n",
       "           6.47812623e-02]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_tensor(softmax_matrix(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Put softmax into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax(matrix, deriv=False):\n",
    "    \"\"\"Return the row-wise softmax matrix or Jacobian tensor of matrix.\n",
    "\n",
    "    :type matrix: ndarray\n",
    "    :param matrix: unnormalized matrix with a row per example\n",
    "    \"\"\"\n",
    "    # Allow passing arrays\n",
    "    if matrix.ndim == 1:\n",
    "        matrix = np.array([matrix])\n",
    "\n",
    "    if deriv:\n",
    "        return jacobian_tensor(softmax_matrix(matrix))\n",
    "    else:\n",
    "        return softmax_matrix(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047],\n",
       "       [ 0.01203764,  0.08894682,  0.24178252,  0.65723302],\n",
       "       [ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  6.22948920e-03,  -1.06822194e-04,  -2.90372829e-04,\n",
       "          -5.83229418e-03],\n",
       "        [ -1.06822194e-04,   1.67499567e-02,  -7.89315184e-04,\n",
       "          -1.58538193e-02],\n",
       "        [ -2.90372829e-04,  -7.89315184e-04,   4.41748369e-02,\n",
       "          -4.30951488e-02],\n",
       "        [ -5.83229418e-03,  -1.58538193e-02,  -4.30951488e-02,\n",
       "           6.47812623e-02]],\n",
       "\n",
       "       [[  1.18927379e-02,  -1.07071001e-03,  -2.91049156e-03,\n",
       "          -7.91153631e-03],\n",
       "        [ -1.07071001e-03,   8.10352810e-02,  -2.15057854e-02,\n",
       "          -5.84587856e-02],\n",
       "        [ -2.91049156e-03,  -2.15057854e-02,   1.83323732e-01,\n",
       "          -1.58907455e-01],\n",
       "        [ -7.91153631e-03,  -5.84587856e-02,  -1.58907455e-01,\n",
       "           2.25277777e-01]],\n",
       "\n",
       "       [[  6.22948920e-03,  -1.06822194e-04,  -2.90372829e-04,\n",
       "          -5.83229418e-03],\n",
       "        [ -1.06822194e-04,   1.67499567e-02,  -7.89315184e-04,\n",
       "          -1.58538193e-02],\n",
       "        [ -2.90372829e-04,  -7.89315184e-04,   4.41748369e-02,\n",
       "          -4.30951488e-02],\n",
       "        [ -5.83229418e-03,  -1.58538193e-02,  -4.30951488e-02,\n",
       "           6.47812623e-02]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(batch, deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Information\n",
    "\n",
    "---\n",
    "\n",
    "The information conveyed about a distribution $P(x)$ by an observation $x$ is inversely proportional to the probability of that observation.\n",
    "\n",
    "$$ I_P(x) := \\frac{1}{P(x)}. $$\n",
    "\n",
    "**Problem:** We'd like the information from two independent observations to be additive. That is, if we observe $x$ and observe $y$, the information gained, $I(x, y)$, should equal $I(x) + I(y)$, but notice that\n",
    "$$ I_P(x) + I_P(y) = \\frac{1}{P(x)} + \\frac{1}{P(y)} \\neq \\frac{1}{P(x)P(y)} = I_P(x, y). $$\n",
    "\n",
    "\n",
    "**Solution:** Use logarithms in our definition, because they turn products into sums.\n",
    "\n",
    "$$ I_P(x) := \\log \\frac{1}{P(x)}. $$\n",
    "\n",
    "Then information is additive.\n",
    "\n",
    "$$ I_P(x) + I_P(y) = \\log \\frac{1}{P(x)} + \\log \\frac{1}{P(y)} = \\log \\frac{1}{P(x)P(y)} = \\log \\frac{1}{P(x, y)} = I_P(x, y) $$\n",
    "\n",
    "where we used (in order), the definition of information, the property of logarithms, and the fact that $x$ and $y$ were assumed to be independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Entropy\n",
    "\n",
    "---\n",
    "\n",
    "The entropy of a discrete distribution $P$ is just the expected information of one observation, $x$, from that distribution\n",
    "\n",
    "$$ H(P) := \\mathbb{E}_{x \\sim P} I_P(x) = \\mathbb{E}_{x \\sim P} \\log \\frac{1}{P(x)} = -\\mathbb{E}_{x \\sim P} \\log P(x) = - \\sum_{x \\in \\text{dom} (P)} P(x) \\log P(x). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross-entropy\n",
    "\n",
    "---\n",
    "\n",
    "The cross-entropy between two discrete distributions $P$ and $Q$ is the expected information about $Q$ garnered from one observation, $x$, drawn from $P$.\n",
    "\n",
    "$$ H(P, Q) = - \\mathbb{E}_{x \\sim P} \\log Q(x) = - \\sum_{x \\in P} P(x) \\log Q(x). $$\n",
    "\n",
    "It's just the entropy of $Q$, but instead weighted by the probabilities of $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross-entropy as a cost\n",
    "\n",
    "---\n",
    "\n",
    "For classification, we have two discrete distributions. Our empirical distribution of correct class labels is given by a one-hot vector $y$ encoding the correct digit of the image. Our model distribution is given by our softmax vector $s(x)$ of class probabilities. We want to force our softmax distribution toward the true distribution (the one-hot vector y). We do this by minimizing the cross-entropy between the two.\n",
    "\n",
    "$$ H(y, s(x)) = - \\mathbb{E}_{y} \\log s(x) = - \\sum_{i=0}^{9} y_i \\log s(x)_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradient of cross-entropy\n",
    "\n",
    "---\n",
    "\n",
    "Since $y$ is our given class label, it doesn't depend on the weights of our network. Therefore, we only need to consider the gradient with respect to $s(x)$, our vector of softmax probabilities. Since $H(y, s(x))$ is a vector-to-scalar function in terms of $s(x)$, we'll have a gradient:\n",
    "\n",
    "$$ \\nabla_{s(x)} H(y, s(x)) = \\left[\\frac{-y_0}{s(x)_0},\\ \\frac{-y_1}{s(x)_1},\\ \\dots,\\ \\frac{-y_9}{s(x)_9} \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sum_cross_entropy(y_matrix, softmax_matrix, deriv=False):\n",
    "    \"\"\"Return sum of row-wise cross-entropies (scalar) or gradients (vector).\n",
    "\n",
    "    :type y_matrix: ndarray\n",
    "    :param y_matrix: rows are one-hot labels for examples in minibatch\n",
    "\n",
    "    :type softmax_matrix: ndarray\n",
    "    :param softmax_matrix: rows are distributions over class predictions\n",
    "    \"\"\"\n",
    "    def cross_entropy(y_vector, softmax_vector, deriv=False):\n",
    "        \"\"\"Return cross-entropy cost (scalar) or gradient (vector).\n",
    "\n",
    "        :type y_vector: ndarray\n",
    "        :param y_vector: one-hot vector indicating the class of an example\n",
    "\n",
    "        :type softmax_vector: ndarray\n",
    "        :param softmax_vector: distribution over class predictions for an example\n",
    "        \"\"\"\n",
    "        if deriv:\n",
    "            return np.array([-y_i / s_i for y_i, s_i\n",
    "                             in zip(y_vector, softmax_vector)])\n",
    "        else:\n",
    "            return -np.sum(y_vector * np.log(softmax_vector))   \n",
    "    if deriv:\n",
    "        gradients = [cross_entropy(y_row, s_row, deriv=True)\n",
    "                     for y_row, s_row in zip(y_matrix, softmax_matrix)]\n",
    "        return np.array(gradients).sum(axis=0)\n",
    "    else:\n",
    "        return sum([cross_entropy(y_row, s_row)\n",
    "                    for y_row, s_row in zip(y_matrix, softmax_matrix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Try cross-entropy on some toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 6],\n",
       "       [2, 4, 5, 6],\n",
       "       [1, 2, 3, 6]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047],\n",
       "       [ 0.01203764,  0.08894682,  0.24178252,  0.65723302],\n",
       "       [ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Labels that well-fit the softmax above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[0, 0, 0, 1],\n",
    "              [0, 0, 0, 1],\n",
    "              [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5640614903019745"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_cross_entropy(y, softmax(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , -3.67121167])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_cross_entropy(y, softmax(batch), deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Labels that poorly fit the softmax\n",
    "\n",
    "- Summed cross-entropy should be higher and its gradient should be steeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[1, 0, 0, 0],\n",
    "              [1, 0, 0, 0],\n",
    "              [1, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.564061490301974"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_cross_entropy(y, softmax(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-402.11373712,    0.        ,    0.        ,    0.        ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_cross_entropy(y, softmax(batch), deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Labels that are so-so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[1, 0, 0, 0],\n",
    "              [0, 0, 0, 1],\n",
    "              [0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.5640614903019738"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_cross_entropy(y, softmax(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-159.52049703,    0.        ,  -21.58875165,   -1.52153036])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_cross_entropy(y, softmax(batch), deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We see that the summed gradient is steepest for that class prediction which is the most incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047],\n",
       "       [ 0.01203764,  0.08894682,  0.24178252,  0.65723302],\n",
       "       [ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We gave a very low softmax probability to digit 0 for each of our 3 examples. Yet, one of the true labels **was** in fact digit zero. That means we must take the most corrective action in the direction of digit zero. That's why our summed gradient is steepest in direction zero, as opposed to the other two directions for whom our predictions were closer to the true class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Chain rule: gradient of cross-entropy times Jacobian of softmax\n",
    "\n",
    "---\n",
    "\n",
    "Let $x$ be a single example. That is, it's a vector input to the softmax layer. To get the gradient of cross-entropy cost (a scalar) with respect to $x$ (a vector input to the softmax layer), we can use the chain rule. Just multiply the gradient of cross-entropy and the Jacobian of softmax\n",
    "\n",
    "$$ \\left[\\frac{-y_0}{s(x)_0},\\ \\frac{-y_1}{s(x)_1},\\ \\dots,\\ \\frac{-y_9}{s(x)_9} \\right]\n",
    "\\begin{bmatrix}\n",
    "    s(x)_0 - s(x)_0^2 & -s(x)_0 s(x)_1 & \\dots  & -s(x)_0 s(x)_9 \\\\\n",
    "    -s(x)_1 s(x)_0 & s(x)_1 - s(x)_1^2 & \\dots  & -s(x)_1 s(x)_9 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    -s(x)_9 s(x)_0 & -s(x)_9 s(x)_1 & \\dots  & s(x)_9 - s(x)_9^2\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "When we do vector-matrix multiplication this way, we're just forming a linear combination of the rows of the Jacobian, where the weights are the entries of the gradient.\n",
    "\n",
    "\\begin{align*}\n",
    "&\\frac{-y_0}{s(x)_0} \\cdot \\left[ s(x)_0 - s(x)_0^2,\\ -s(x)_0 s(x)_1,\\ \\dots,\\  -s(x)_0 s(x)_9 \\right] \\\\ \\\\\n",
    "+ &\\frac{-y_1}{s(x)_1} \\cdot \\left[ -s(x)_1 s(x)_0,\\ s(x)_1 - s(x)_1^2,\\ \\dots,\\ -s(x)_1 s(x)_9 \\right] \\\\ \\\\\n",
    "&\\dots \\\\ \\\\\n",
    "+ &\\frac{-y_9}{s(x)_9} \\cdot \\left[ -s(x)_9 s(x)_0,\\ -s(x)_9 s(x)_1,\\ \\dots,\\  s(x)_9 - s(x)_9^2 \\right] \\\\ \\\\\n",
    "\\end{align*}\n",
    "\n",
    "if we cancel terms we get...\n",
    "\n",
    "\\begin{align*}\n",
    "&y_0 \\cdot \\left[ s(x)_0 - 1,\\ s(x)_1,\\ \\dots,\\  s(x)_9 \\right] \\\\ \\\\\n",
    "+ &y_1 \\cdot \\left[ s(x)_0,\\ s(x)_1 - 1,\\ \\dots,\\ s(x)_9 \\right] \\\\ \\\\\n",
    "&\\dots \\\\ \\\\\n",
    "+ &y_9 \\cdot \\left[ s(x)_0,\\ s(x)_1,\\ \\dots,\\  s(x)_9 - 1 \\right] \\\\ \\\\\n",
    "\\end{align*}\n",
    "\n",
    "if we reframe as a vector-matrix product again we get...\n",
    "\n",
    "$$ \\left[y_0,\\ y_1,\\ \\dots,\\ y_9 \\right]\n",
    "\\begin{bmatrix}\n",
    "    s(x)_0 - 1 & s(x)_1 & s(x)_2 & \\dots  & s(x)_9 \\\\\n",
    "    s(x)_0 & s(x)_1 - 1 & s(x)_2 & \\dots  & s(x)_9 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s(x)_0 & s(x)_1 & s(x)_2 & \\dots  & s(x)_9 - 1\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "We can frame this as a vector-matrix product of the vector $y$ with the matrix whose rows are identically our softmax distribution, minus the identity matrix\n",
    "\n",
    "$$ \\left[y_0,\\ y_1,\\ \\dots,\\ y_9 \\right] \\left(\n",
    "\\begin{bmatrix}\n",
    "    s(x)_0 & s(x)_1 & s(x)_2 & \\dots  & s(x)_9 \\\\\n",
    "    s(x)_0 & s(x)_1 & s(x)_2 & \\dots  & s(x)_9 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s(x)_0 & s(x)_1 & s(x)_2 & \\dots  & s(x)_9\n",
    "\\end{bmatrix} \n",
    "-\n",
    "I_{9 \\times 9} \\right)\n",
    "$$\n",
    "\n",
    "but since $y$ is a one-hot vector, we just get the row corresponding to the correct class. **This is just the softmax vector $s(x)$ of our input vector $x$, with 1 subtracted from the correct class**. Furthermore, there's an easy way to subtract 1 from the correct class. Just subtract our one-hot vector $y$ (1 for the correct class, and 0 elsewhere), from our softmax vector $s(x)$.\n",
    "\n",
    "$$s(x) - y_{\\text{onehot}}$$\n",
    "\n",
    "So for each example in our minibatch, the error with respect to the input vector to the softmax layer is just\n",
    "\n",
    "$$s(x) - y_{\\text{onehot}}$$.\n",
    "\n",
    "For the entire minibatch, since softmax is row-wise, and cross-entropy is row-wise, our rows are independent, so we just sum the gradient of each row.\n",
    "\n",
    "That means, the error with respect to the input of the softmax layer is\n",
    "\n",
    "$$\\delta_{\\text{input}} = \\sum_{\\text{examples}} s(x)^{\\text{example}} - y_{\\text{onehot}}^{\\text{example}}.$$\n",
    "\n",
    "If we subtract our $y$ matrix whose rows are one-hot vectors, from our softmax matrix $s(x)$ whose rows are softmax vectors, and then sum the rows of the resulting matrix, we'll have achieved the above effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(batch, onehot_labels, deriv=False):\n",
    "    \"\"\"\"Return sum cross-entropy cost or delta_input for final layer.\n",
    "    \n",
    "    :type y_matrix: ndarray\n",
    "    :param y_matrix: rows are one-hot vectors for correct label\n",
    "    \n",
    "    :type x_matrix: ndarray\n",
    "    :param x_matrix: rows will be softmaxed and cross-entropied with \n",
    "                     the rows of y_matrix.\n",
    "    \"\"\"\n",
    "    if deriv:\n",
    "        return (softmax(batch) - onehot_labels).sum(axis=0)\n",
    "    else:\n",
    "        return sum_cross_entropy(onehot_labels, softmax(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047],\n",
       "       [ 0.01203764,  0.08894682,  0.24178252,  0.65723302],\n",
       "       [ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Good labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[0, 0, 0, 1],\n",
    "              [0, 0, 0, 1],\n",
    "              [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.490403605775839"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy(y, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.47536689,  -7.47536689, -10.47536689, -16.57389934])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy(y, batch, deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These tell us how the cross-entropy cost changes if we increase the weighted input to each of our four output nodes. We see that the cost declines if we have more weighted input to the fourth node, because that node contains all of our true class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bad labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[1, 0, 0, 0],\n",
    "              [1, 0, 0, 0],\n",
    "              [1, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.490403605775839"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy(y, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, since our true labels are all digit 0, we have a negative cost from increasing the weighted input to node zero of the final layer. Also notice that the expected cost decrease is much larger. That's because our softmax values are quite small for output zero in each of our examples. This shows that very wrong softmax predictions lead to very strong corrective gradients when using cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.57389934,  -7.47536689, -10.47536689, -17.47536689])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy(y, batch, deriv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047],\n",
       "       [ 0.01203764,  0.08894682,  0.24178252,  0.65723302],\n",
       "       [ 0.00626879,  0.01704033,  0.04632042,  0.93037047]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Mixed labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[1, 0, 0, 0],\n",
    "              [0, 0, 0, 1],\n",
    "              [0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.490403605775839"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy(y, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.1748777 ,  -7.47536689, -10.1748777 , -17.1748777 ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_cross_entropy(y, batch, deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the mixed labels, we can decrease cost through two channels, by increasing the weighted input to node zero, or increasing the weighted input to node 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layers = [\n",
    "  # (size, nonlin, keep_prob),\n",
    "    (784, linear, 0.8),  # input\n",
    "    (500, relu, 0.5),\n",
    "    (500, relu, 0.5),\n",
    "    (10, softmax_cross_entropy, 1.0),  # output\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### MLP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"Store a list of Layer() instances.\"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"Initialize list of Layer() instances.\"\"\"\n",
    "        self._layers = [\n",
    "            Layer(prev_layer[0], *layer)\n",
    "            for prev_layer, layer in zip(layers[:-1], layers[1:])\n",
    "        ]\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Return a string of layers of the network.\"\"\"\n",
    "        return '\\n'.join(str(layer) for layer in self)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of layers.\"\"\"\n",
    "        return len(self._layers)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return Layer() at index.\"\"\"\n",
    "        return self._layers[index]\n",
    "\n",
    "    def forward(self, batch, labels):\n",
    "        \"\"\"Return cost of minibatch.\n",
    "        \n",
    "        :type batch: ndarray\n",
    "        :param batch: minibatch whose rows are examples\n",
    "        \n",
    "        :type labels: ndarray\n",
    "        :param labels: each row a one-hot label for an example\n",
    "        \"\"\"\n",
    "        for layer in self:\n",
    "            batch = layer.forward(batch, labels)\n",
    "        return batch\n",
    "\n",
    "    def backward(self, batch, labels):\n",
    "        \"\"\"Forward pass followed by weight update.\"\"\"\n",
    "        # Forward\n",
    "        for layer in self[:-1]:\n",
    "            batch = layer.forward(batch, labels)\n",
    "        batch = softmax(batch)\n",
    "        \n",
    "        \n",
    "        # Backward\n",
    "        delta = self[-1].nonlin(batch, labels, deriv=True)\n",
    "        return delta\n",
    "        for layer in self[::-1]:\n",
    "            layer.backward()\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        \"\"\"Repeat backward method to train the network.\"\"\"\n",
    "        # delta_output = None\n",
    "        for _ in range(n_epochs):\n",
    "            self.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Store and update incoming weights to a layer.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, size, nonlin, keep_prob):\n",
    "        \"\"\"Initialize weight and Jacobian matrices.\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: size of previous layer\n",
    "\n",
    "        :type size: int\n",
    "        :param size: size of this layer\n",
    "\n",
    "        :type nonlin: function\n",
    "        :param nonlin: nonlinearity for this layer\n",
    "\n",
    "        :type keep_prob: float\n",
    "        :param keep_prob: keep probability for dropout\n",
    "        \"\"\"\n",
    "        # Store parameters\n",
    "        self.n_in = n_in\n",
    "        self.size = size\n",
    "        self.nonlin = nonlin\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # Initialize weights and Jacobian of weights\n",
    "        self.weights = np.random.rand(n_in, size) * np.sqrt(2.0 / n_in)\n",
    "        self.jacobian = np.zeros(self.weights.shape)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Return string representation of layer.\"\"\"\n",
    "        return 'Layer(%d, %s, %.1f)' \\\n",
    "               % (self.size, self.nonlin.__name__, self.keep_prob)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"Return layer activation, computed from previous layer activation.\"\"\"\n",
    "        self.z = x.dot(self.weights)  # weighted input\n",
    "        self.a = self.nonlin(self.z, labels)  # activation\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta_next, w_next, learning_rate=0.1):\n",
    "        \"\"\"Get delta for weighted input to layer given delta for next layer.\n",
    "\n",
    "        :type delta_next: ndarray\n",
    "        :param delta_netx: derivative of cost w.r.t. input to next layer\n",
    "\n",
    "        :type W_next: ndarray\n",
    "        :param W_next: weight matrix for next layer (needed for bringing delta back)\n",
    "        \"\"\"\n",
    "        self.delta = w_next.T.dot(delta_next) * self.nonlin(self.Z, deriv=True)\n",
    "        self.dJdW = self.x.dot(self.delta)\n",
    "        self.w -= learning_rate * self.dJdW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP(layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
