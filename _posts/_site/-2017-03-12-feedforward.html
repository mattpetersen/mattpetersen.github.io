<p>In this post we’ll feed a dataset forward through a neural network to get an output. For an output, let’s predict whether or not it’s going to rain on a given day. What factors could predict rain? Some good guesses would be temperature, humidity, cloud cover, and wind speed. Let’s gather some data about these factors over a few days.</p>

<p><img src="images/rain/data_raw_350_325.png" alt="Data raw" /></p>

<p>Consider a network with three layers.</p>

<p><img src="http://www.texample.net/media/tikz/examples/PNG/neural-network.png" alt="Neural Network" /></p>

<p>This network will predict rain. The red node will output zero for sunny, and one for rainy. Each green node represents one input feature. Since there are four green nodes, we’ll have four input features. The four input features we use are</p>

<ul>
  <li>Temperature</li>
  <li>Humidity</li>
  <li>Cloud cover</li>
  <li>Wind speed</li>
</ul>

<p>These input features are defined by our dataset. Typically our data are stored in a table, where</p>

<ul>
  <li>Each row is an observation</li>
  <li>Each column is a feature</li>
</ul>

<h1 id="feedforward">Feedforward</h1>

<p><strong>Input layer:</strong></p>

<p>First, we input four feature values into the green nodes.</p>

<ul>
  <li>Temperature = 22 Celsius</li>
  <li>Humidity = 80 percent</li>
  <li>Cloud cover = 70 percent</li>
  <li>Wind speed = 10 km/h</li>
</ul>

<p>This represents one observation (one row) of our data matrix. Each value is that of a feature (a column) of our data matrix.</p>

<p><strong>Note (normalize the data):</strong> We won’t feed in these exact values. Always scale the feature values to be centered at zero with unit variance. This is called normalizing.</p>

<p><strong>Example (normalize the data):</strong> We normalize the temperature column by first subtracting the mean temperature from every temperature, and then dividing each temperature by the sample standard deviation of temperature. We can write this as</p>

<dtex>\text{Temperature}_{\text{normalized}} = \frac{t - \mu_t}{\sigma_t}</dtex>

<p>Here, temperature is a column vector (a feature column of our data matrix), and</p>

<ul>
  <li><script type="math/tex">t</script> = column vector of temperature values (our data)</li>
  <li><script type="math/tex">\mu_t</script> = column vector where every element is the same, the average temperature</li>
  <li><script type="math/tex">\sigma_t</script> = sample standard deviation of temperature</li>
</ul>

<p><strong>Hidden Layer:</strong></p>

<p>To compute the values of the blue nodes we do a row-vector times matrix multiplication. The row vector holds our four normalized feature values (temperature, humidity, cloud cover, and wind speed). The matrix holds the weights connecting the input layer to the hidden layer. The weight matrix will have</p>

<ul>
  <li><script type="math/tex">4</script> rows (one for each input feature)</li>
  <li><script type="math/tex">5</script> columns (one for each hidden feature)</li>
</ul>

<p>Multiplying a <script type="math/tex">1 \times 4</script> row vector with a <script type="math/tex">4 \times 5</script> weight matrix yields a <script type="math/tex">1 \times 5</script> hidden vector. We see that our four normalized input features (temperature, humidity, cloud cover, and wind speed) have been converted into five hidden features.</p>

<p><strong>Note:</strong> We don’t actually know what these five hidden features represent. <em>The entire goal of training a neural network is to <strong>learn</strong> what features the hidden nodes should represent.</em> Neural networks should be thought of as automatic feature generators. Before neural networks, experts would spend years studying the nuances of datasets and trying to manually engineer features that would allow simple classifiers to perform well. Later, we’ll see that the output layer of a neural network is in fact just a simple classifier, but the key is that <em>it’s applied to learned features.</em></p>

